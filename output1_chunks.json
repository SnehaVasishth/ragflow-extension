[
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "Figure 1: The Transformer - model architecture.\nOutput\nProbabilities\nSoftmax\nAdd&Norm\nFeed\nForward\nAdd&Norm\nAdd&Norm\nMulti-Head\nFeed\n Attention \nForward\nNx\nAdd&Norm\nNx\nAdd&Norm\nMasked\n Multi-Head \nMulti-Head\n Attention \n Attention\nPositional\nPositional\nEncoding\n Encoding\nInput \n Output\nEmbedding\nEmbedding\nInputs \nOutputs\n(shifted right)",
    "content_ltks": "figur 1 the transform model architectur output probabl softmax add norm feedforward add norm add norm multihead feed attent forward nx add norm nx add norm mask multihead multihead attent attent posit posit encod encod input output embed embed input output shift right",
    "content_sm_ltks": "figur 1 the transform model architectur output probabl softmax add norm feedforward add norm add norm multihead feed attent forward nx add norm nx add norm mask multihead multihead attent attent posit posit encod encod input output embed embed input output shift right",
    "image": "<PIL.Image.Image image mode=RGB size=656x976 at 0x74CE6C774CA0>",
    "doc_type_kwd": "image",
    "page_num_int": [
      3
    ],
    "position_int": [
      [
        3,
        195,
        414,
        69,
        395
      ]
    ],
    "top_int": [
      69
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nMulti-Head Attention\nLinear\n[Concat\nScaledDot-Product\nAttention\nQ\n",
    "content_ltks": "figur 2 left scale dot product attent right multihead attent consist of sever multihead attent linear concat scaleddot product attent q",
    "content_sm_ltks": "figur 2 left scale dot product attent right multihead attent consist of sever multihead attent linear concat scaleddot product attent q",
    "image": "<PIL.Image.Image image mode=RGB size=368x505 at 0x74CE6C774730>",
    "doc_type_kwd": "image",
    "page_num_int": [
      4
    ],
    "position_int": [
      [
        4,
        344,
        467,
        71,
        239
      ]
    ],
    "top_int": [
      71
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "(1)\n attention layers running in parallel.\nQKT\nAttention(Q, K, V) = softmax(\n)V\n√dk",
    "content_ltks": "1 attent layer run in parallel qkt attent qk v softmax vdk",
    "content_sm_ltks": "1 attent layer run in parallel qkt attent qk v softmax vdk",
    "image": "<PIL.Image.Image image mode=RGB size=1198x530 at 0x74CE6C774AC0>",
    "doc_type_kwd": "image",
    "page_num_int": [
      4
    ],
    "position_int": [
      [
        4,
        106,
        505,
        285,
        462
      ]
    ],
    "top_int": [
      285
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "(2)\nFFN(α) = max(0, cW1 + b1)W2 + b2",
    "content_ltks": "2 ffn α max 0 cw1 b1 w2 b2",
    "content_sm_ltks": "2 ffn α max 0 cw1 b1 w2 b2",
    "image": "<PIL.Image.Image image mode=RGB size=835x35 at 0x74CE6C774580>",
    "doc_type_kwd": "image",
    "page_num_int": [
      5
    ],
    "position_int": [
      [
        5,
        226,
        504,
        519,
        531
      ]
    ],
    "top_int": [
      519
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "PE(pos,i) = sin(pos / 100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)",
    "content_ltks": "pepo isin po 100002i dmodel pepo 2i 1 copo 100002i dmodel",
    "content_sm_ltks": "pepo isin po 100002i dmodel pepo 2i 1 copo 100002i dmodel",
    "image": "<PIL.Image.Image image mode=RGB size=491x99 at 0x74CE6C7747F0>",
    "doc_type_kwd": "image",
    "page_num_int": [
      6
    ],
    "position_int": [
      [
        6,
        223,
        387,
        293,
        326
      ]
    ],
    "top_int": [
      293
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "(3)\nlrate = dmodsa min(step_num-0.5 , step_num warmup_steps-1.5)",
    "content_ltks": "3 lrate dmodsa min step _ num 0 5 step _ num warmup _ step 15",
    "content_sm_ltks": "3 lrate dmodsa min step _ num 0 5 step _ num warmup _ step 15",
    "image": "<PIL.Image.Image image mode=RGB size=1035x52 at 0x74CE6C774D90>",
    "doc_type_kwd": "image",
    "page_num_int": [
      7
    ],
    "position_int": [
      [
        7,
        159,
        504,
        563,
        580
      ]
    ],
    "top_int": [
      563
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "the word ‘making'. Different colors represent different heads. Best viewed in color.\nthe verb ‘making', completing the phrase ‘making...more difficult'. Attentions here shown only for \nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of \nFigure 3: An example of the attention mechanism following long-distance dependencies in the\n.S.=\nD\nmaJ\nAmer\ngovemm\n?",
    "content_ltks": "the word make differ color repres differ head best view in color the verb make complet the phrase make more difficult attent here shown onli for encod self attent in layer 5 of 6 mani of the attent head attend toa distant depend of figur 3 an exampl of the attent mechan follow long distanc depend in the sd maj amer govemm",
    "content_sm_ltks": "the word make differ color repres differ head best view in color the verb make complet the phrase make more difficult attent here shown onli for encod self attent in layer 5 of 6 mani of the attent head attend toa distant depend of figur 3 an exampl of the attent mechan follow long distanc depend in the sd maj amer govemm",
    "image": "<PIL.Image.Image image mode=RGB size=1167x614 at 0x74CE6C774850>",
    "doc_type_kwd": "image",
    "page_num_int": [
      13
    ],
    "position_int": [
      [
        13,
        116,
        505,
        99,
        303
      ]
    ],
    "top_int": [
      99
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": " and 6. Note that the attentions are very sharp for this word.\nFull attentions for head 5. Bottom: Isolated attentions from just the word “its′ for attention heads 5\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nlication\nsing\nfect\nould\nS\nEO\nappl\nS\nthis\nwe\nM\nb\ne\nS\n perfect\nwhat\napplication\nmissing\nbe\nshould\nb\njust\nThe\nM\nthis\nopinion\n<EOS>\nnever\n<pad>\napplication\nsing\nshould \n.0\npe\ne\nC\nMMM\n  \nE\ne\napplication\nperfect\nThe\nwill\n$\nshould\nwhat\n missing\nopinion\nLaw\nnever\nb\n<EOS>\nb\n.S\nW\n.n\n<pad>",
    "content_ltks": "and 6 note that the attent are veri sharp forthi word full attent forhead 5 bottom isol attent from just the word it for attent head 5 figur 4 two attent head also in layer 5 of 6 appar involv in anaphora resolut top licat sing fect ould seo appl s thi wem be s perfect what applic miss be shouldb just them thi opinion eo never pad applic sing should 0 peec mmm ee applic perfect the will should what miss opinion law never beo bsw n pad",
    "content_sm_ltks": "and 6 note that the attent are veri sharp forthi word full attent forhead 5 bottom isol attent from just the word it for attent head 5 figur 4 two attent head also in layer 5 of 6 appar involv in anaphora resolut top licat sing fect ould seo appl s thi wem be s perfect what applic miss be shouldb just them thi opinion eo never pad applic sing should 0 peec mmm ee applic perfect the will should what miss opinion law never beo bsw n pad",
    "image": "<PIL.Image.Image image mode=RGB size=1142x1358 at 0x74CE6C775030>",
    "doc_type_kwd": "image",
    "page_num_int": [
      14
    ],
    "position_int": [
      [
        14,
        119,
        500,
        162,
        614
      ]
    ],
    "top_int": [
      162
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "at layer 5 of 6. The heads clearly learned to perform different tasks.\n sentence. We give two such examples above, from two different heads from the encoder self-attention\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nplication\nshould\nec\ne\nP\nm\npinous\n#\nb\n.S\nwhat\nect\napplication\n.s\ning\nS\nperfe\nmissi\nEO\nopir\nolication\nnissind\nshould\nperfect\nM\nhe\napp\nare\ne\nWill\nperfect\nver\nbut\n.$\napplication\npInous\nb\nare\nwe\ning\nhe\nthis\nmissi\nney\nO\nopir",
    "content_ltks": "at layer 5 of 6 the head clearli learn to perform differ task sentenc we give two such exampl abov from two differ head from the encod self attent figur 5 mani of the attent head exhibit behaviour that seem relat to the structur of the plicat should ece pm pinou bs what ect applic sing s perf missi eo opir olic nissind should perfect mhe app are e will perfect ver but applic pinou bare we ing he thi missi ney o opir",
    "content_sm_ltks": "at layer 5 of 6 the head clearli learn to perform differ task sentenc we give two such exampl abov from two differ head from the encod self attent figur 5 mani of the attent head exhibit behaviour that seem relat to the structur of the plicat should ece pm pinou bs what ect applic sing s perf missi eo opir olic nissind should perfect mhe app are e will perfect ver but applic pinou bare we ing he thi missi ney o opir",
    "image": "<PIL.Image.Image image mode=RGB size=1137x1259 at 0x74CE6C774DC0>",
    "doc_type_kwd": "image",
    "page_num_int": [
      15
    ],
    "position_int": [
      [
        15,
        119,
        498,
        175,
        595
      ]
    ],
    "top_int": [
      175
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "<table><caption>Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operationsfor different layer types. n is the sequence length, d is the representation dimension, k is the kernelsize of convolutions and r the size of the neighborhood in restricted self-attention.</caption>\n<tr><td  > Layer Type</td><td  >Complexity per Layer</td><td  > Sequential</td><td  >Maximum Path Length</td></tr>\n<tr><td></td><td></td><td  >Operations</td><td></td></tr>\n<tr><td  >Self-Attention</td><td  >O(n2 . d)</td><td  >0(1)</td><td  >0(1)</td></tr>\n<tr><td  >Recurrent</td><td  >O(n . d²)</td><td  >O(n)</td><td  >O(n)</td></tr>\n<tr><td  >Convolutional</td><td  >O(k · n · d²)</td><td  >0(1)</td><td  >O(logk(n))</td></tr>\n<tr><td  >Self-Attention (restricted)</td><td  >O(r . n · d)</td><td  >0(1)</td><td  >O(n /r)</td></tr>\n</table>",
    "content_ltks": "tabl 1 maximum pathlength per layer complex and minimum numberof sequenti operationsfor differ layer typen is the sequenc length d is the represent dimens k is the kernels of convolut andr the sizeof the neighborhood in restrict self attent layer type complex per layer sequenti maximum pathlength oper self attent o n2 d 01 01 recurr ond ² onon convolut okn d ² 01 o logk n self attent restrictor nd 01 onr",
    "content_sm_ltks": "tabl 1 maximum pathlength per layer complex and minimum numberof sequenti operationsfor differ layer typen is the sequenc length d is the represent dimens k is the kernels of convolut andr the sizeof the neighborhood in restrict self attent layer type complex per layer sequenti maximum pathlength oper self attent o n2 d 01 01 recurr ond ² onon convolut okn d ² 01 o logk n self attent restrictor nd 01 onr",
    "image": "<PIL.Image.Image image mode=RGB size=1134x230 at 0x74CE6C7755A0>",
    "doc_type_kwd": "image",
    "page_num_int": [
      6
    ],
    "position_int": [
      [
        6,
        116,
        494,
        111,
        188
      ]
    ],
    "top_int": [
      111
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "<table><caption>Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</caption>\n<tr><td></td><td colspan=2 >BLEU</td><td  >Training Cost (FLOPs)</td></tr>\n<tr><td  >Model</td><td  >EN-DE</td><td  >EN-FR</td><td  >EN-DE EN-FR</td></tr>\n<tr><td  >ByteNet [18]</td><td  >23.75</td><td></td><td></td></tr>\n<tr><td  >Deep-Att + PosUnk [39]</td><td></td><td  >39.2</td><td  >1.0 · 1020</td></tr>\n<tr><td  >GNMT + RL [38]</td><td  >24.6</td><td  >39.92</td><td  >2.3 ·1019 1.4 ·1020</td></tr>\n<tr><td  >ConvS2S [9]</td><td  >25.16</td><td  >40.46</td><td  >9.6 · 1018 1.5 · 1020</td></tr>\n<tr><td  >MoE [32]</td><td  >26.03</td><td  >40.56</td><td  >2.0 · 1019 1.2 · 1020</td></tr>\n<tr><td  >Deep-Att + PosUnk Ensemble [39]</td><td></td><td  >40.4</td><td  >8.0 · 1020</td></tr>\n<tr><td  >GNMT + RL Ensemble [38] ConvS2S Ensemble [9]</td><td  >26.30 26.36</td><td  >41.16 41.29</td><td  >1.8 · 1020 1.1 · 1021 7.7 · 1019 1.2 · 1021</td></tr>\n<tr><td  >Transformer (base model)</td><td  >27.3</td><td  >38.1</td><td  >3.3· 1018</td></tr>\n<tr><td  >Transformer (big)</td><td  >28.4</td><td  >41.8</td><td  >2.3· 1019</td></tr>\n</table>",
    "content_ltks": "tabl 2 the transform achiev better bleu score than previou state of the art model on theenglish to german and english to french newstest2014 testat a fraction of the train cost bleu train cost flop model en deen fren deen fr bytenet 18 23 75 deep att posunk 39 39 21 0 1020 gnmt rl 38 24 6 39 92 23 1019 1 4 1020 convs2 9 25 16 40 46 96 1018 15 1020 moe 32 26 03 40 56 20 1019 12 1020 deep att posunk ensembl 39 40 48 0 1020 gnmt rl ensembl 38 convs2 ensembl 9 26 30 26 36 41 16 41 2918 1020 11 1021 77 1019 12 1021 transform base model 273 38 13 3 1018 transform big 28 4 41 8 23 1019",
    "content_sm_ltks": "tabl 2 the transform achiev better bleu score than previou state of the art model on theenglish to german and english to french newstest2014 testat a fraction of the train cost bleu train cost flop model en deen fren deen fr bytenet 18 23 75 deep att posunk 39 39 21 0 1020 gnmt rl 38 24 6 39 92 23 1019 1 4 1020 convs2 9 25 16 40 46 96 1018 15 1020 moe 32 26 03 40 56 20 1019 12 1020 deep att posunk ensembl 39 40 48 0 1020 gnmt rl ensembl 38 convs2 ensembl 9 26 30 26 36 41 16 41 2918 1020 11 1021 77 1019 12 1021 transform base model 273 38 13 3 1018 transform big 28 4 41 8 23 1019",
    "image": "<PIL.Image.Image image mode=RGB size=1063x453 at 0x74CE95952050>",
    "doc_type_kwd": "image",
    "page_num_int": [
      8
    ],
    "position_int": [
      [
        8,
        128,
        482,
        93,
        244
      ]
    ],
    "top_int": [
      93
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "<table><caption>Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the basemodel. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.</caption>\n<tr><th  rowspan=2 >base</th><th  rowspan=2 >N</th><th  >dmodel dff</th><th  >h</th><th  >dk</th><th  >d~</th><th  >Pdrop</th><th  >Els</th><th  >train steps</th><th  >PPL (dev)</th><th  >BLEU (dev)</th><th  >params ×106</th></tr>\n<tr><td  >512 2048</td><td  >8</td><td  >64</td><td  >64</td><td  >0.1</td><td  >0.1</td><td  >100K</td><td  >4.92</td><td  >25.8</td><td  >65</td></tr>\n<tr><td  >(A)</td><td></td><td></td><td  >4 16 32</td><td  >512 128 32 16</td><td  >512 128 32 16</td><td></td><td></td><td></td><td  >5.29 5.00 4.91 5.01</td><td  >24.9 25.5 25.8 25.4</td><td></td></tr>\n<tr><td  >(B)</td><td></td><td></td><td></td><td  >16 32</td><td></td><td></td><td></td><td></td><td  >5.16 5.01</td><td  >25.1 25.4</td><td  >58 60</td></tr>\n<tr><td  >(C)</td><td  >4 8</td><td  >256 1024 1024 4096</td><td></td><td  >32 128</td><td  >32 128</td><td></td><td></td><td></td><td  >6.11 5.19 4.88 5.75 4.66 5.12 4.75</td><td  >23.7 25.3 25.5 24.5 26.0 25.4 26.2</td><td  >36 50 80 28 168 53 90</td></tr>\n<tr><td  >(D)</td><td></td><td></td><td></td><td></td><td></td><td  >0.0 0.2</td><td  >0.0 0.2</td><td></td><td  >5.77 4.95 4.67 5.47</td><td  >24.6 25.5 25.3 25.7</td><td></td></tr>\n<tr><td  >(E)</td><td colspan=9 >positional embedding instead of sinusoids 4.92</td><td  >25.7</td><td></td></tr>\n<tr><td  >big</td><td></td><td  >1024 4096</td><td  >16</td><td></td><td></td><td  >0.3</td><td></td><td  >300K</td><td  >4.33</td><td  >26.4</td><td  >213</td></tr>\n</table>",
    "content_ltks": "tabl 3 variat on the transform architectur unlist valu are ident to those of the basemodel all metric are on the english to german translat develop set newstest2013 list perplex are per wordpiec accord to our byte pair encod and should not be compar toper word perplex basen dmodel dff hdk d pdrop el train step ppl dev bleu dev param 106 512 2048 8 64 64 01 01 100k 4 92 258 65 a4 16 32 512 128 32 16 512 128 32 16 5 29 500 4 91 5 01 24 9 25 5 258 25 4 b 16 32 5 16 5 01 25 1 25 4 58 60 c4 8 256 1024 1024 4096 32 128 32 128 611 519 4 88 5 75 4 66 5 12 4 75 23 7 25 3 25 5 24 5 26 0 25 4 26 2 36 50 80 28 168 53 90 d 0 0 02 0 0 02 5 77 4 95 4 67 5 47 24 6 25 5 25 3 25 7 e posit embed instead of sinusoid 4 92 25 7 big 1024 4096 160 3 300k 4 33 26 4 213",
    "content_sm_ltks": "tabl 3 variat on the transform architectur unlist valu are ident to those of the basemodel all metric are on the english to german translat develop set newstest2013 list perplex are per wordpiec accord to our byte pair encod and should not be compar toper word perplex basen dmodel dff hdk d pdrop el train step ppl dev bleu dev param 106 512 2048 8 64 64 01 01 100k 4 92 258 65 a4 16 32 512 128 32 16 512 128 32 16 5 29 500 4 91 5 01 24 9 25 5 258 25 4 b 16 32 5 16 5 01 25 1 25 4 58 60 c4 8 256 1024 1024 4096 32 128 32 128 611 519 4 88 5 75 4 66 5 12 4 75 23 7 25 3 25 5 24 5 26 0 25 4 26 2 36 50 80 28 168 53 90 d 0 0 02 0 0 02 5 77 4 95 4 67 5 47 24 6 25 5 25 3 25 7 e posit embed instead of sinusoid 4 92 25 7 big 1024 4096 160 3 300k 4 33 26 4 213",
    "image": "<PIL.Image.Image image mode=RGB size=1210x773 at 0x74CE6E2E97B0>",
    "doc_type_kwd": "image",
    "page_num_int": [
      9
    ],
    "position_int": [
      [
        9,
        106,
        509,
        128,
        385
      ]
    ],
    "top_int": [
      128
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "content_with_weight": "<table><caption> Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)</caption>\n<tr><th  >Parser</th><th  >Training</th><th  >WSJ 23 F1</th></tr>\n<tr><th  >Vinyals & Kaiser el al. (2014) [37] Petrov et al. (2006) [29]</th><th  >WSJ only, discriminative WSJ only, discriminative</th><th  >88.3 90.4</th></tr>\n<tr><th  >Zhu et al. (2013) [40]</th><th  >WSJ only, discriminative</th><th  >90.4</th></tr>\n<tr><th  >Dyer et al. (2016) [8]</th><th  >WSJ only, discriminative</th><th  >91.7</th></tr>\n<tr><th  >Transformer (4 layers)</th><th  >WSJ only, discriminative</th><th  >91.3</th></tr>\n<tr><th  >Zhu et al. (2013) [40]</th><th  >semi-supervised</th><th  >91.3</th></tr>\n<tr><th  >Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37]</th><th  >semi-supervised semi-supervised semi-supervised</th><th  >91.3 92.1 92.1</th></tr>\n<tr><th  >Transformer (4 layers)</th><th  >semi-supervised</th><th  >92.7</th></tr>\n<tr><th  >Luong et al. (2015) [23] Dyer et al. (2016) [8]</th><th  >multi-task generative</th><th  >93.0 93.3</th></tr>\n</table>",
    "content_ltks": "tabl 4 the transform gener well to english constitu pars result are on section 23of wsj parser train wsj 23 f1 vinyal kaiser el al 2014 37 petrov etal 2006 29 wsj onli discrimin wsj onli discrimin 88 3 90 4 zhu etal 2013 40 wsj onli discrimin 90 4 dyer etal 2016 8 wsj onli discrimin 91 7 transform 4 layer wsj onli discrimin 913 zhu etal 2013 40 semi supervis 913 huang harper 2009 14 mccloski etal 2006 26 vinyal kaiser el al 2014 37 semi supervis semi supervis semi supervis 913 92 1 92 1 transform 4 layer semi supervis 927 luong etal 2015 23 dyer etal 2016 8 multitask gener 93 0 93 3",
    "content_sm_ltks": "tabl 4 the transform gener well to english constitu pars result are on section 23of wsj parser train wsj 23 f1 vinyal kaiser el al 2014 37 petrov etal 2006 29 wsj onli discrimin wsj onli discrimin 88 3 90 4 zhu etal 2013 40 wsj onli discrimin 90 4 dyer etal 2016 8 wsj onli discrimin 91 7 transform 4 layer wsj onli discrimin 913 zhu etal 2013 40 semi supervis 913 huang harper 2009 14 mccloski etal 2006 26 vinyal kaiser el al 2014 37 semi supervis semi supervis semi supervis 913 92 1 92 1 transform 4 layer semi supervis 927 luong etal 2015 23 dyer etal 2016 8 multitask gener 93 0 93 3",
    "image": "<PIL.Image.Image image mode=RGB size=977x437 at 0x74CE6F122E30>",
    "doc_type_kwd": "image",
    "page_num_int": [
      9
    ],
    "position_int": [
      [
        9,
        142,
        468,
        424,
        570
      ]
    ],
    "top_int": [
      424
    ]
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "important_kwd": [
      "abstract",
      "总结",
      "概括",
      "summary",
      "summarize"
    ],
    "important_tks": "abstract 总结 概括 summary summarize",
    "image": "<PIL.Image.Image image mode=RGB size=978x737 at 0x74CE6E29BEB0>",
    "page_num_int": [
      1
    ],
    "position_int": [
      [
        1,
        143,
        469,
        361,
        373
      ]
    ],
    "top_int": [
      361
    ],
    "content_with_weight": "the dominant sequence transduction models are based on complex recurrent or",
    "content_ltks": "the domin sequenc transduct model are base on complex recurr or",
    "content_sm_ltks": "the domin sequenc transduct model are base on complex recurr or"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x3848 at 0x74CE6E2BC0D0>",
    "page_num_int": [
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      1,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2,
      2
    ],
    "position_int": [
      [
        1,
        143,
        543,
        373,
        383
      ],
      [
        1,
        141,
        541,
        382,
        396
      ],
      [
        1,
        143,
        543,
        394,
        406
      ],
      [
        1,
        143,
        543,
        405,
        416
      ],
      [
        1,
        141,
        541,
        415,
        429
      ],
      [
        1,
        141,
        541,
        425,
        439
      ],
      [
        1,
        141,
        541,
        435,
        449
      ],
      [
        1,
        143,
        543,
        448,
        460
      ],
      [
        1,
        143,
        543,
        459,
        471
      ],
      [
        1,
        143,
        543,
        470,
        481
      ],
      [
        1,
        143,
        543,
        481,
        493
      ],
      [
        1,
        143,
        543,
        492,
        504
      ],
      [
        1,
        142,
        542,
        502,
        516
      ],
      [
        1,
        143,
        543,
        514,
        526
      ],
      [
        1,
        124,
        524,
        547,
        558
      ],
      [
        1,
        105,
        505,
        569,
        583
      ],
      [
        1,
        106,
        506,
        582,
        594
      ],
      [
        1,
        119,
        519,
        599,
        613
      ],
      [
        1,
        106,
        506,
        611,
        622
      ],
      [
        1,
        106,
        506,
        620,
        634
      ],
      [
        1,
        105,
        505,
        630,
        642
      ],
      [
        1,
        107,
        507,
        641,
        652
      ],
      [
        1,
        107,
        507,
        651,
        662
      ],
      [
        1,
        106,
        506,
        660,
        672
      ],
      [
        1,
        106,
        506,
        671,
        683
      ],
      [
        1,
        107,
        507,
        682,
        691
      ],
      [
        1,
        119,
        519,
        691,
        703
      ],
      [
        1,
        116,
        516,
        698,
        715
      ],
      [
        1,
        107,
        507,
        732,
        743
      ],
      [
        2,
        106,
        506,
        73,
        85
      ],
      [
        2,
        107,
        507,
        85,
        97
      ],
      [
        2,
        107,
        507,
        96,
        107
      ],
      [
        2,
        107,
        507,
        112,
        123
      ],
      [
        2,
        106,
        506,
        122,
        136
      ],
      [
        2,
        105,
        505,
        132,
        147
      ],
      [
        2,
        106,
        506,
        144,
        158
      ],
      [
        2,
        106,
        506,
        155,
        169
      ],
      [
        2,
        107,
        507,
        167,
        178
      ],
      [
        2,
        108,
        508,
        178,
        189
      ],
      [
        2,
        107,
        507,
        189,
        200
      ],
      [
        2,
        106,
        506,
        203,
        217
      ],
      [
        2,
        107,
        507,
        216,
        227
      ],
      [
        2,
        107,
        507,
        227,
        238
      ],
      [
        2,
        107,
        507,
        238,
        249
      ],
      [
        2,
        107,
        507,
        254,
        265
      ],
      [
        2,
        107,
        507,
        265,
        277
      ],
      [
        2,
        107,
        507,
        276,
        288
      ],
      [
        2,
        107,
        507,
        287,
        298
      ],
      [
        2,
        105,
        505,
        317,
        331
      ],
      [
        2,
        107,
        507,
        346,
        358
      ],
      [
        2,
        107,
        507,
        358,
        369
      ],
      [
        2,
        106,
        506,
        368,
        382
      ],
      [
        2,
        106,
        506,
        378,
        390
      ],
      [
        2,
        107,
        507,
        391,
        402
      ],
      [
        2,
        106,
        506,
        401,
        412
      ],
      [
        2,
        107,
        507,
        412,
        424
      ],
      [
        2,
        107,
        507,
        424,
        434
      ],
      [
        2,
        107,
        507,
        434,
        445
      ],
      [
        2,
        107,
        507,
        450,
        462
      ],
      [
        2,
        107,
        507,
        462,
        473
      ],
      [
        2,
        107,
        507,
        472,
        484
      ],
      [
        2,
        107,
        507,
        483,
        495
      ],
      [
        2,
        105,
        505,
        497,
        511
      ],
      [
        2,
        105,
        505,
        509,
        523
      ],
      [
        2,
        107,
        507,
        522,
        533
      ],
      [
        2,
        107,
        507,
        538,
        549
      ],
      [
        2,
        106,
        506,
        547,
        561
      ],
      [
        2,
        106,
        506,
        559,
        571
      ],
      [
        2,
        106,
        506,
        570,
        581
      ],
      [
        2,
        105,
        505,
        602,
        614
      ],
      [
        2,
        107,
        507,
        630,
        642
      ],
      [
        2,
        105,
        505,
        639,
        653
      ],
      [
        2,
        107,
        507,
        652,
        664
      ],
      [
        2,
        106,
        506,
        663,
        676
      ],
      [
        2,
        106,
        506,
        673,
        687
      ],
      [
        2,
        107,
        507,
        690,
        702
      ],
      [
        2,
        107,
        507,
        701,
        712
      ],
      [
        2,
        106,
        506,
        713,
        725
      ]
    ],
    "top_int": [
      373,
      382,
      394,
      405,
      415,
      425,
      435,
      448,
      459,
      470,
      481,
      492,
      502,
      514,
      547,
      569,
      582,
      599,
      611,
      620,
      630,
      641,
      651,
      660,
      671,
      682,
      691,
      698,
      732,
      73,
      85,
      96,
      112,
      122,
      132,
      144,
      155,
      167,
      178,
      189,
      203,
      216,
      227,
      238,
      254,
      265,
      276,
      287,
      317,
      346,
      358,
      368,
      378,
      391,
      401,
      412,
      424,
      434,
      450,
      462,
      472,
      483,
      497,
      509,
      522,
      538,
      547,
      559,
      570,
      602,
      630,
      639,
      652,
      663,
      673,
      690,
      701,
      713
    ],
    "content_with_weight": "convolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\n*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product atention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nf Work performed while at Google Brain.\n+Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht-1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for allinput and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]\nHere, the encoder maps an input sequence of symbol representations (1,., n) to a sequence\nof continuous representations z = (z1, ., Zn). Given z, the decoder then generates an output\nsequence (y1 ., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.",
    "content_ltks": "convolut neural network that includ an encod anda decod thebest perform model also connect the encod and decod through an attent mechan we propos anew simpl network architectur the transform base sole on attent mechan dispens with recurr and convolut entir experi on two machin translat task show these model tobe superior in qualiti while be more paralleliz and requir significantli less time to train our model achiev 28 4 bleu on the wmt 2014 english to german translat task improv over the exist best result includ ensembl by over 2 bleu on the wmt 2014 english to french translat task our model establish anew singl model state of the art bleu score of 41 8 after train for 35 day on eight gpu a small fraction of the train cost of thebest model from the literatur we show that the transform gener well to other task by appli it success to english constitu pars both with larg and limit train data introduct recurr neural network long shortterm memori 13 and gate recurr 7 neural network inparticular havebeen firmli establish a state of the art approach in sequenc model and equal contribut list order is random jakob propos replac rnn with self attent and start the effort to evalu thi idea ashish with illia design and implement thefirst transform model and ha been crucial involv in everi aspect ofthi work noam propos scale dot product atent multihead attent and the paramet free posit represent and becam theother person involv in nearli everi detail niki design implement tune and evalu countless model variant in our origin codebas and tensor2tensor llion also experi with novel model variant wa respons for our initi codebas and effici infer and visual lukasz and aidan spent countless long day design variou partof and implement tensor2tensor replac our earlier codebas greatli improv result and massiv acceler our research f work perform while at googl brain work perform while at googl research 31st confer on neural inform process system nip 2017 longbeach causa transduct problem sucha languag model and machin translat 352 5 numer effort have sinc continu to push the boundari of recurr languag model and encod decod architectur 38 24 15 recurr model typic factor comput along the symbol posit of the input and output sequenc align the posit to stepin comput time they genera sequenc of hidden state hta a function of the previou hidden state ht 1 and the input for posit t thi inher sequenti natur preclud parallel within train exampl which becom critic at longer sequenc length a memori constraint limit batch across exampl recent work ha achiev signific improv in comput effici through factor trick 21 and condit comput 32 while also improv model perform in case of the latter the fundament constraint of sequenti comput howev remain attent mechan have becom an integr partof compel sequenc model and transduc tion modelin variou task allow model of depend without regard to their distanc in the input or output sequenc 2 19 in all buta few case 27 howev such attent mechan are use inconjunct witha recurr network inthi work we propos the transforma model architectur eschew recurr and instead reli entir onan attent mechan to draw global depend between input and output the transform allow for significantli more parallel and can reach anew state of the artin translat qualiti after be train fora littl a twelv hour on eight p100 gpu 2background the goal of reduc sequenti comput also form the foundat of the extend neural gpu 16 bytenet 18 and convs2 9 allof which use convolut neural network a basic build block comput hidden representin parallel for allinput and output posit in these model the numberof oper requir to relat signal from two arbitrari input or output posit growin the distanc between posit linearli for convs2 and logarithm for bytenet thi make it more difficult to learn depend between distant posit 12 in the transform thi is reduc toa constant numberof oper albeit at the cost of reduc effect resolut dueto averag attent weight posit an effect we counteract with multihead attent a describ in section 32 self attent sometim call intra attent isan attent mechan relat differ posit ofa singl sequenc in orderto computa represent of the sequenc self attent ha been use success ina varieti of task includ read comprehens abstract summar textual entail and learn task independ sentenc represent 4 27 28 22 end to end memori network are base ona recurr attent mechan instead of sequenc align recurr and havebeen shown to perform well on simpl languag question answer and languag model task 34 to thebest ofour knowledg howev the transform is thefirst transduct model reli entir on self attent to comput represent ofit input and output without use sequenc align rnn or convolut in thefollow section we will describ the transform motiv self attent and discu it advantag over model sucha 1718 and 9 3model architectur most competit neural sequenc transduct model have an encod decod structur 52 35 here the encod map an input sequenc of symbol represent 1 nto a sequenc of continu represent z z1 zn given zthe decod then gener an output sequenc y1 ym of symbol one element ata time at each step the modelis autoregress 10 consum the previous gener symbol a addit input when gener the next the transform follow thi overal architectur use stack self attent and point wise fulli connect layer for both the encod and decod shown in the left and right half of figur 1 respect",
    "content_sm_ltks": "convolut neural network that includ an encod anda decod thebest perform model also connect the encod and decod through an attent mechan we propos anew simpl network architectur the transform base sole on attent mechan dispens with recurr and convolut entir experi on two machin translat task show these model tobe superior in qualiti while be more paralleliz and requir significantli less time to train our model achiev 28 4 bleu on the wmt 2014 english to german translat task improv over the exist best result includ ensembl by over 2 bleu on the wmt 2014 english to french translat task our model establish anew singl model state of the art bleu score of 41 8 after train for 35 day on eight gpu a small fraction of the train cost of thebest model from the literatur we show that the transform gener well to other task by appli it success to english constitu pars both with larg and limit train data introduct recurr neural network long shortterm memori 13 and gate recurr 7 neural network inparticular havebeen firmli establish a state of the art approach in sequenc model and equal contribut list order is random jakob propos replac rnn with self attent and start the effort to evalu thi idea ashish with illia design and implement thefirst transform model and ha been crucial involv in everi aspect ofthi work noam propos scale dot product atent multihead attent and the paramet free posit represent and becam theother person involv in nearli everi detail niki design implement tune and evalu countless model variant in our origin codebas and tensor2tensor llion also experi with novel model variant wa respons for our initi codebas and effici infer and visual lukasz and aidan spent countless long day design variou partof and implement tensor2tensor replac our earlier codebas greatli improv result and massiv acceler our research f work perform while at googl brain work perform while at googl research 31st confer on neural inform process system nip 2017 longbeach causa transduct problem sucha languag model and machin translat 352 5 numer effort have sinc continu to push the boundari of recurr languag model and encod decod architectur 38 24 15 recurr model typic factor comput along the symbol posit of the input and output sequenc align the posit to stepin comput time they genera sequenc of hidden state hta a function of the previou hidden state ht 1 and the input for posit t thi inher sequenti natur preclud parallel within train exampl which becom critic at longer sequenc length a memori constraint limit batch across exampl recent work ha achiev signific improv in comput effici through factor trick 21 and condit comput 32 while also improv model perform in case of the latter the fundament constraint of sequenti comput howev remain attent mechan have becom an integr partof compel sequenc model and transduc tion modelin variou task allow model of depend without regard to their distanc in the input or output sequenc 2 19 in all buta few case 27 howev such attent mechan are use inconjunct witha recurr network inthi work we propos the transforma model architectur eschew recurr and instead reli entir onan attent mechan to draw global depend between input and output the transform allow for significantli more parallel and can reach anew state of the artin translat qualiti after be train fora littl a twelv hour on eight p100 gpu 2background the goal of reduc sequenti comput also form the foundat of the extend neural gpu 16 bytenet 18 and convs2 9 allof which use convolut neural network a basic build block comput hidden representin parallel for allinput and output posit in these model the numberof oper requir to relat signal from two arbitrari input or output posit growin the distanc between posit linearli for convs2 and logarithm for bytenet thi make it more difficult to learn depend between distant posit 12 in the transform thi is reduc toa constant numberof oper albeit at the cost of reduc effect resolut dueto averag attent weight posit an effect we counteract with multihead attent a describ in section 32 self attent sometim call intra attent isan attent mechan relat differ posit ofa singl sequenc in orderto computa represent of the sequenc self attent ha been use success ina varieti of task includ read comprehens abstract summar textual entail and learn task independ sentenc represent 4 27 28 22 end to end memori network are base ona recurr attent mechan instead of sequenc align recurr and havebeen shown to perform well on simpl languag question answer and languag model task 34 to thebest ofour knowledg howev the transform is thefirst transduct model reli entir on self attent to comput represent ofit input and output without use sequenc align rnn or convolut in thefollow section we will describ the transform motiv self attent and discu it advantag over model sucha 1718 and 9 3model architectur most competit neural sequenc transduct model have an encod decod structur 52 35 here the encod map an input sequenc of symbol represent 1 nto a sequenc of continu represent z z1 zn given zthe decod then gener an output sequenc y1 ym of symbol one element ata time at each step the modelis autoregress 10 consum the previous gener symbol a addit input when gener the next the transform follow thi overal architectur use stack self attent and point wise fulli connect layer for both the encod and decod shown in the left and right half of figur 1 respect"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1203x1314 at 0x74CE6E2AE7D0>",
    "page_num_int": [
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3
    ],
    "position_int": [
      [
        3,
        107,
        508,
        442,
        452
      ],
      [
        3,
        106,
        507,
        464,
        474
      ],
      [
        3,
        106,
        507,
        474,
        486
      ],
      [
        3,
        106,
        507,
        486,
        497
      ],
      [
        3,
        106,
        507,
        497,
        509
      ],
      [
        3,
        106,
        507,
        508,
        519
      ],
      [
        3,
        104,
        505,
        516,
        530
      ],
      [
        3,
        104,
        505,
        529,
        543
      ],
      [
        3,
        146,
        547,
        559,
        570
      ],
      [
        3,
        106,
        507,
        571,
        582
      ],
      [
        3,
        105,
        506,
        580,
        592
      ],
      [
        3,
        107,
        508,
        592,
        604
      ],
      [
        3,
        107,
        508,
        604,
        615
      ],
      [
        3,
        106,
        507,
        614,
        626
      ],
      [
        3,
        104,
        505,
        624,
        638
      ]
    ],
    "top_int": [
      442,
      464,
      474,
      486,
      497,
      508,
      516,
      529,
      559,
      571,
      580,
      592,
      604,
      614,
      624
    ],
    "content_with_weight": "3.1Encoder and DecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(c + Sublayer(x)), where Sublayer(c) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the twoDecoder:\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.",
    "content_ltks": "3 1encod and decoderstack encod the encod is compos ofa stack ofn 6 ident layer each layer ha two sub layer thefirst isa multihead self attent mechan and the second isa simpl posit wise fulli connect feedforward network we employ a residu connect 11 around each of thetwo sub layer follow by layer normal 1 that is the output ofeach sub layer is layernorm c sublay x where sublay c is the function implement by the sub layer itself to facilit these residu connect all sub layer in the model a wella the embed layer produc output of dimens dmodel 512 the decod is also compos ofa stack ofn 6 ident layer inaddit to the twodecod sub layer in each encod layer the decod insert a third sub layer which perform multihead attent over the output of the encod stack similarto the encod we employ residu connect around each of the sub layer follow by layer normal we also modifi the self attent sub layer in the decod stack to prevent posit from attend to subsequ posit thi mask combin with fact that the output embed are offset by one posit ensur that the predict for positi can depend onlion the known output at posit lessthan i",
    "content_sm_ltks": "3 1encod and decoderstack encod the encod is compos ofa stack ofn 6 ident layer each layer ha two sub layer thefirst isa multihead self attent mechan and the second isa simpl posit wise fulli connect feedforward network we employ a residu connect 11 around each of thetwo sub layer follow by layer normal 1 that is the output ofeach sub layer is layernorm c sublay x where sublay c is the function implement by the sub layer itself to facilit these residu connect all sub layer in the model a wella the embed layer produc output of dimens dmodel 512 the decod is also compos ofa stack ofn 6 ident layer inaddit to the twodecod sub layer in each encod layer the decod insert a third sub layer which perform multihead attent over the output of the encod stack similarto the encod we employ residu connect around each of the sub layer follow by layer normal we also modifi the self attent sub layer in the decod stack to prevent posit from attend to subsequ posit thi mask combin with fact that the output embed are offset by one posit ensur that the predict for positi can depend onlion the known output at posit lessthan i"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1205x1141 at 0x74CE6E2AFAF0>",
    "page_num_int": [
      3,
      3,
      3,
      3,
      3,
      4,
      4,
      4,
      4,
      4,
      4,
      4
    ],
    "position_int": [
      [
        3,
        106,
        507,
        656,
        667
      ],
      [
        3,
        105,
        507,
        677,
        691
      ],
      [
        3,
        105,
        507,
        688,
        702
      ],
      [
        3,
        104,
        506,
        698,
        712
      ],
      [
        3,
        107,
        508,
        712,
        723
      ],
      [
        4,
        204,
        606,
        107,
        113
      ],
      [
        4,
        176,
        578,
        124,
        136
      ],
      [
        4,
        194,
        596,
        138,
        145
      ],
      [
        4,
        175,
        576,
        146,
        155
      ],
      [
        4,
        194,
        596,
        160,
        165
      ],
      [
        4,
        183,
        584,
        166,
        178
      ],
      [
        4,
        178,
        579,
        189,
        198
      ]
    ],
    "top_int": [
      656,
      677,
      688,
      698,
      712,
      107,
      124,
      138,
      146,
      160,
      166,
      189
    ],
    "content_with_weight": "3.2Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\nMatMu\nSoftMax\n↑\nMask (opt.)\n↑\nScale\nMatMul",
    "content_ltks": "3 2attent an attent function can be describ amapa queri anda setof keyvalu pair toan output where the queri keyvalu and output are all vector the output is computa a weight sum of thevalu where the weight assign to each valu is comput bya compat function of the queri with the correspond key matmu softmax mask opt scale matmul",
    "content_sm_ltks": "3 2attent an attent function can be describ amapa queri anda setof keyvalu pair toan output where the queri keyvalu and output are all vector the output is computa a weight sum of thevalu where the weight assign to each valu is comput bya compat function of the queri with the correspond key matmu softmax mask opt scale matmul"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1203x1113 at 0x74CE6E2AD990>",
    "page_num_int": [
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      4
    ],
    "position_int": [
      [
        4,
        106,
        507,
        318,
        329
      ],
      [
        4,
        106,
        507,
        337,
        349
      ],
      [
        4,
        105,
        506,
        347,
        361
      ],
      [
        4,
        106,
        507,
        359,
        371
      ],
      [
        4,
        106,
        507,
        370,
        382
      ],
      [
        4,
        104,
        505,
        384,
        398
      ],
      [
        4,
        106,
        507,
        397,
        409
      ],
      [
        4,
        106,
        507,
        409,
        420
      ],
      [
        4,
        107,
        508,
        471,
        482
      ],
      [
        4,
        106,
        507,
        483,
        494
      ]
    ],
    "top_int": [
      318,
      337,
      347,
      359,
      370,
      384,
      397,
      409,
      471,
      483
    ],
    "content_with_weight": "3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V. We compute\nthe matrix of outputs as:\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor",
    "content_ltks": "321 scale dot product attent we call our particular attent scale dot product attent figur 2 the input consist of queri and key of dimens dk and valu of dimens dv we comput the dot product of the queri with all key divid each by dk and appli a softmax function to obtain the weighton thevalu in practic we comput the attent function ona setof queri simultan pack togeth into a matrix qthe key and valu are also pack togeth into matrix kand vwe comput the matrix of output a thetwo most commonli use attent function are addit attent 2 and dot product multi plic attent dot product attent is ident to our algorithm except for the scalefactor",
    "content_sm_ltks": "321 scale dot product attent we call our particular attent scale dot product attent figur 2 the input consist of queri and key of dimens dk and valu of dimens dv we comput the dot product of the queri with all key divid each by dk and appli a softmax function to obtain the weighton thevalu in practic we comput the attent function ona setof queri simultan pack togeth into a matrix qthe key and valu are also pack togeth into matrix kand vwe comput the matrix of output a thetwo most commonli use attent function are addit attent 2 and dot product multi plic attent dot product attent is ident to our algorithm except for the scalefactor"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x1041 at 0x74CE6E2AF190>",
    "page_num_int": [
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      4
    ],
    "position_int": [
      [
        4,
        132,
        532,
        497,
        506
      ],
      [
        4,
        106,
        506,
        506,
        518
      ],
      [
        4,
        106,
        506,
        518,
        529
      ],
      [
        4,
        105,
        505,
        528,
        538
      ],
      [
        4,
        105,
        505,
        543,
        555
      ],
      [
        4,
        105,
        505,
        555,
        566
      ],
      [
        4,
        105,
        505,
        565,
        579
      ],
      [
        4,
        103,
        503,
        571,
        591
      ]
    ],
    "top_int": [
      497,
      506,
      518,
      528,
      543,
      555,
      565,
      571
    ],
    "content_with_weight": ". Additive attention computes the compatibility function using a feed-forward network withO1Vdk\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of d [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients To counteract tiseffect, we scale the dot products bys",
    "content_ltks": "addit attent comput the compat function usea feedforward network witho1vdk asingl hidden layer while thetwo are similar in theoret complex dot product attent is much faster and more space effici in practic sinc it can be implement use highli optim matrix multipl code while for small valu of dk thetwo mechan perform similarli addit attent outperform dot product attent without scale for larger valu ofd 3 we suspect that for larg valu of dk the dot product grow larg in magnitud push the softmax function into region where it ha extrem small gradient to counteract tiseffect we scale the dot product by",
    "content_sm_ltks": "addit attent comput the compat function usea feedforward network witho1vdk asingl hidden layer while thetwo are similar in theoret complex dot product attent is much faster and more space effici in practic sinc it can be implement use highli optim matrix multipl code while for small valu of dk thetwo mechan perform similarli addit attent outperform dot product attent without scale for larger valu ofd 3 we suspect that for larg valu of dk the dot product grow larg in magnitud push the softmax function into region where it ha extrem small gradient to counteract tiseffect we scale the dot product by"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1198x1480 at 0x74CE6E2AF790>",
    "page_num_int": [
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      4,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5
    ],
    "position_int": [
      [
        4,
        105,
        505,
        603,
        614
      ],
      [
        4,
        105,
        505,
        623,
        634
      ],
      [
        4,
        106,
        505,
        635,
        646
      ],
      [
        4,
        106,
        505,
        646,
        657
      ],
      [
        4,
        107,
        506,
        657,
        669
      ],
      [
        4,
        107,
        506,
        668,
        679
      ],
      [
        4,
        107,
        506,
        679,
        690
      ],
      [
        4,
        119,
        518,
        699,
        711
      ],
      [
        4,
        107,
        506,
        711,
        725
      ],
      [
        5,
        107,
        506,
        74,
        85
      ],
      [
        5,
        108,
        507,
        85,
        97
      ],
      [
        5,
        185,
        584,
        113,
        129
      ],
      [
        5,
        223,
        622,
        129,
        143
      ],
      [
        5,
        104,
        504,
        166,
        182
      ],
      [
        5,
        105,
        505,
        178,
        193
      ],
      [
        5,
        106,
        505,
        197,
        208
      ],
      [
        5,
        107,
        506,
        208,
        220
      ],
      [
        5,
        107,
        506,
        219,
        231
      ]
    ],
    "top_int": [
      603,
      623,
      635,
      646,
      657,
      668,
      679,
      699,
      711,
      74,
      85,
      113,
      129,
      166,
      178,
      197,
      208,
      219
    ],
    "content_with_weight": "3.2.2Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to d, dk and d, dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding d, -dimensional\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean O and variance 1. Then their dot product, q · k = >1 qiki, has mean O and variance dk.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, , headn)WO\nwhere head; = Attention(QW, KWK, VW)\nWhere the projections are parameter matrices WQ E Rdnodelxds , WK E Rdnodel xds, WV E Rdnodel Xdy\nand WO E IRhdy xdmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = d = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.",
    "content_ltks": "32 2multi head attent instead of performa singl attent function with dmodel dimension keyvalu and queri we found it benefici to linearli project the queri key and valu h time with differ learn linear project tod dk andd dimens respect on each of these project version of queri key and valu we then perform the attent function in parallel yield d dimension output valu these are concaten and onc again project result in the final valua depict in figur 2 4to illustr whi the dot product get larg assum that the compon ofq andk are independ random variabl with mean oand varianc 1 then their dot product qk 1 qiki ha mean oand varianc dk multihead attent allow the model to jointli attend to inform from differ represent subspac at differ posit witha singl attent head averag inhibit thi multihead qk v concat head1 headn wo where head attent qw kwk vw where the project are paramet matrix wq e rdnodelxd wk e rdnodel xd wve rdnodel xdi and woe irhdi xdmodel inthi work we employ h 8 parallel attent layer or head foreach of these we use dkd dmodel h 64 dueto the reduc dimens ofeach head the total comput cost is similarto that of singl head attent with full dimension",
    "content_sm_ltks": "32 2multi head attent instead of performa singl attent function with dmodel dimension keyvalu and queri we found it benefici to linearli project the queri key and valu h time with differ learn linear project tod dk andd dimens respect on each of these project version of queri key and valu we then perform the attent function in parallel yield d dimension output valu these are concaten and onc again project result in the final valua depict in figur 2 4to illustr whi the dot product get larg assum that the compon ofq andk are independ random variabl with mean oand varianc 1 then their dot product qk 1 qiki ha mean oand varianc dk multihead attent allow the model to jointli attend to inform from differ represent subspac at differ posit witha singl attent head averag inhibit thi multihead qk v concat head1 headn wo where head attent qw kwk vw where the project are paramet matrix wq e rdnodelxd wk e rdnodel xd wve rdnodel xdi and woe irhdi xdmodel inthi work we employ h 8 parallel attent layer or head foreach of these we use dkd dmodel h 64 dueto the reduc dimens ofeach head the total comput cost is similarto that of singl head attent with full dimension"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1115x1347 at 0x74CE6E2AEAD0>",
    "page_num_int": [
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5
    ],
    "position_int": [
      [
        5,
        107,
        479,
        241,
        252
      ],
      [
        5,
        105,
        477,
        257,
        271
      ],
      [
        5,
        133,
        504,
        279,
        290
      ],
      [
        5,
        142,
        514,
        289,
        301
      ],
      [
        5,
        143,
        514,
        301,
        312
      ],
      [
        5,
        142,
        514,
        311,
        322
      ],
      [
        5,
        143,
        514,
        322,
        334
      ],
      [
        5,
        133,
        504,
        336,
        348
      ],
      [
        5,
        143,
        514,
        348,
        359
      ],
      [
        5,
        143,
        514,
        359,
        370
      ],
      [
        5,
        144,
        515,
        370,
        379
      ],
      [
        5,
        133,
        504,
        383,
        395
      ],
      [
        5,
        142,
        514,
        394,
        406
      ],
      [
        5,
        142,
        514,
        404,
        418
      ],
      [
        5,
        143,
        514,
        415,
        427
      ],
      [
        5,
        143,
        514,
        428,
        439
      ]
    ],
    "top_int": [
      241,
      257,
      279,
      289,
      301,
      311,
      322,
      336,
      348,
      359,
      370,
      383,
      394,
      404,
      415,
      428
    ],
    "content_with_weight": "3.2.3Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n· In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n· The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n· Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to --oo) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.",
    "content_ltks": "32 3applic of attent in our model the transform us multihead attent in three differ way in encod decod attent layer the queri come from the previou decod layer and the memori key and valu come from the output of the encod thi allow everi posit in the decod to attend over all posit in the input sequenc thi mimic the typic encod decod attent mechan in sequenc to sequenc model sucha 38 29 the encod contain self attent layer ina self attent layer allof the keyvalu and queri come from the same place inthi case the output of the previou layer in the encod each posit in the encod can attend to all posit in the previou layer of the encod similarli self attent layer in the decod allow each posit in the decod to attend to all posit in the decod upto and includ that posit we needto prevent leftward inform flowin the decod to preserv the autoregress properti we implement thi insid of scale dot product attent by mask outset tooo all valu in the input of the softmax which correspond to illeg connect see figur 2",
    "content_sm_ltks": "32 3applic of attent in our model the transform us multihead attent in three differ way in encod decod attent layer the queri come from the previou decod layer and the memori key and valu come from the output of the encod thi allow everi posit in the decod to attend over all posit in the input sequenc thi mimic the typic encod decod attent mechan in sequenc to sequenc model sucha 38 29 the encod contain self attent layer ina self attent layer allof the keyvalu and queri come from the same place inthi case the output of the previou layer in the encod each posit in the encod can attend to all posit in the previou layer of the encod similarli self attent layer in the decod allow each posit in the decod to attend to all posit in the decod upto and includ that posit we needto prevent leftward inform flowin the decod to preserv the autoregress properti we implement thi insid of scale dot product attent by mask outset tooo all valu in the input of the softmax which correspond to illeg connect see figur 2"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x1040 at 0x74CE6E2AFA90>",
    "page_num_int": [
      5,
      5,
      5,
      5,
      5,
      5,
      5,
      5
    ],
    "position_int": [
      [
        5,
        107,
        507,
        450,
        462
      ],
      [
        5,
        107,
        507,
        471,
        481
      ],
      [
        5,
        108,
        508,
        481,
        493
      ],
      [
        5,
        106,
        506,
        491,
        505
      ],
      [
        5,
        107,
        507,
        538,
        549
      ],
      [
        5,
        107,
        507,
        548,
        560
      ],
      [
        5,
        104,
        504,
        557,
        573
      ],
      [
        5,
        106,
        506,
        572,
        583
      ]
    ],
    "top_int": [
      450,
      471,
      481,
      491,
      538,
      548,
      557,
      572
    ],
    "content_with_weight": "3.3Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff=2048.",
    "content_ltks": "3 3posit wise feedforward network inaddit to attent sub layer each of the layer in our encod and decod contain a fulli connect feedforward network which is appli to each posit separ and ident thi consist of two linear transform witha relu activin between while the linear transform are the same across differ posit they use differ paramet from layer to layer anoth way of describ thi isa two convolut with kernel size 1 the dimension of input and output is dmodel 512 and the inner layer ha dimension dff 2048",
    "content_sm_ltks": "3 3posit wise feedforward network inaddit to attent sub layer each of the layer in our encod and decod contain a fulli connect feedforward network which is appli to each posit separ and ident thi consist of two linear transform witha relu activin between while the linear transform are the same across differ posit they use differ paramet from layer to layer anoth way of describ thi isa two convolut with kernel size 1 the dimension of input and output is dmodel 512 and the inner layer ha dimension dff 2048"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x953 at 0x74CE6E2AE6B0>",
    "page_num_int": [
      5,
      5,
      5,
      5,
      5,
      5
    ],
    "position_int": [
      [
        5,
        106,
        506,
        593,
        604
      ],
      [
        5,
        106,
        506,
        612,
        626
      ],
      [
        5,
        107,
        507,
        625,
        637
      ],
      [
        5,
        107,
        507,
        636,
        647
      ],
      [
        5,
        108,
        508,
        647,
        658
      ],
      [
        5,
        105,
        505,
        656,
        670
      ]
    ],
    "top_int": [
      593,
      612,
      625,
      636,
      647,
      656
    ],
    "content_with_weight": "3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.",
    "content_ltks": "34 embed and softmax similarli to other sequenc transduct model we use learn embed to convert the input token and output token to vector of dimens dmodel we also use the usual learn linear transfor mation and softmax function to convert the decod output to predict nexttoken probabl in our model we share the same weight matrix between thetwo embed layer and the pre softmax linear transform similarto 30 in the embed layer we multipli those weight by dmodel",
    "content_sm_ltks": "34 embed and softmax similarli to other sequenc transduct model we use learn embed to convert the input token and output token to vector of dimens dmodel we also use the usual learn linear transfor mation and softmax function to convert the decod output to predict nexttoken probabl in our model we share the same weight matrix between thetwo embed layer and the pre softmax linear transform similarto 30 in the embed layer we multipli those weight by dmodel"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x2951 at 0x74CE6E2BC2B0>",
    "page_num_int": [
      5,
      5,
      5,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      6,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7
    ],
    "position_int": [
      [
        5,
        105,
        505,
        679,
        693
      ],
      [
        5,
        107,
        507,
        700,
        712
      ],
      [
        5,
        108,
        508,
        713,
        725
      ],
      [
        6,
        107,
        507,
        213,
        224
      ],
      [
        6,
        105,
        505,
        221,
        235
      ],
      [
        6,
        105,
        505,
        231,
        245
      ],
      [
        6,
        107,
        507,
        246,
        257
      ],
      [
        6,
        105,
        505,
        260,
        273
      ],
      [
        6,
        105,
        505,
        332,
        346
      ],
      [
        6,
        105,
        505,
        343,
        357
      ],
      [
        6,
        106,
        506,
        355,
        367
      ],
      [
        6,
        105,
        505,
        364,
        377
      ],
      [
        6,
        107,
        507,
        375,
        387
      ],
      [
        6,
        107,
        507,
        393,
        405
      ],
      [
        6,
        107,
        507,
        404,
        415
      ],
      [
        6,
        106,
        506,
        415,
        427
      ],
      [
        6,
        106,
        506,
        424,
        438
      ],
      [
        6,
        106,
        506,
        453,
        465
      ],
      [
        6,
        104,
        504,
        475,
        489
      ],
      [
        6,
        106,
        506,
        487,
        499
      ],
      [
        6,
        105,
        505,
        498,
        512
      ],
      [
        6,
        107,
        507,
        510,
        522
      ],
      [
        6,
        106,
        506,
        520,
        532
      ],
      [
        6,
        105,
        505,
        535,
        549
      ],
      [
        6,
        107,
        507,
        548,
        560
      ],
      [
        6,
        106,
        506,
        563,
        577
      ],
      [
        6,
        107,
        507,
        576,
        587
      ],
      [
        6,
        107,
        507,
        587,
        598
      ],
      [
        6,
        107,
        507,
        597,
        609
      ],
      [
        6,
        106,
        506,
        609,
        620
      ],
      [
        6,
        107,
        507,
        619,
        631
      ],
      [
        6,
        106,
        506,
        629,
        641
      ],
      [
        6,
        108,
        508,
        646,
        658
      ],
      [
        6,
        106,
        506,
        657,
        669
      ],
      [
        6,
        106,
        506,
        668,
        679
      ],
      [
        6,
        107,
        507,
        679,
        691
      ],
      [
        6,
        107,
        507,
        690,
        702
      ],
      [
        6,
        106,
        506,
        701,
        712
      ],
      [
        6,
        107,
        507,
        712,
        724
      ],
      [
        7,
        107,
        507,
        74,
        85
      ],
      [
        7,
        107,
        507,
        85,
        97
      ],
      [
        7,
        107,
        507,
        101,
        113
      ],
      [
        7,
        107,
        507,
        113,
        124
      ],
      [
        7,
        107,
        507,
        123,
        135
      ],
      [
        7,
        106,
        506,
        134,
        146
      ],
      [
        7,
        107,
        507,
        146,
        156
      ],
      [
        7,
        105,
        505,
        154,
        168
      ],
      [
        7,
        107,
        507,
        167,
        179
      ],
      [
        7,
        105,
        505,
        177,
        189
      ],
      [
        7,
        107,
        507,
        194,
        206
      ],
      [
        7,
        106,
        506,
        204,
        218
      ],
      [
        7,
        105,
        505,
        214,
        228
      ],
      [
        7,
        106,
        506,
        226,
        237
      ]
    ],
    "top_int": [
      679,
      700,
      713,
      213,
      221,
      231,
      246,
      260,
      332,
      343,
      355,
      364,
      375,
      393,
      404,
      415,
      424,
      453,
      475,
      487,
      498,
      510,
      520,
      535,
      548,
      563,
      576,
      587,
      597,
      609,
      619,
      629,
      646,
      657,
      668,
      679,
      690,
      701,
      712,
      74,
      85,
      101,
      113,
      123,
      134,
      146,
      154,
      167,
      177,
      194,
      204,
      214,
      226
    ],
    "content_with_weight": "3.5Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 : 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(1, ., &n) to another sequence of equal length (z1, ., Zn), with ci, Z E Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n /r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n /k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.",
    "content_ltks": "3 5posit encod sinc our model contain no recurr and no convolut in order for the model to make useof the order of the sequenc we must inject some inform about the rel or absolut posit of the token in the sequenc tothi end we add posit encod to the input embed at the bottom of the encod and decod stack the posit encod have the same dimens dmodel a the embed sothat thetwo can be sum there are mani choic of posit encod learn and fix 9 inthi work we use sine and cosin function of differ frequenc where pois the posit andi is the dimens that is each dimens of the posit encod correspond toa sinusoid the wavelength forma geometr progress from 2 π to 10000 2 π we chose thi function becaus we hypothes it would allow the model to easili learn to attend by rel posit sinc for ani fix offset k pepo k can be represa a linear function of pepo we also experi with use learn posit embed 9 instead and found that thetwo version produc nearli ident result see tabl 3 row ewe chose the sinusoid version becaus it may allow the model to extrapol to sequenc length longer than the one encount dure train 4whi self attent inthi section we compar variou aspect of self attent layer to the recurr and convolu tional layer commonli usefor map one variabl length sequenc of symbol represent 1 nto anoth sequenc of equal length z1 zn with ci ze rd sucha a hidden layer ina typic sequenc transduct encod or decod motiv our useof self attent we consid three desideratum one is the total comput complex per layer anoth is the amount of comput that can be parallel a measur by the minimum numberof sequenti oper requir the third is the pathlength between long rang depend in the network learn long rang depend isa key challeng in mani sequenc transduct task one key factor affect the abil to learn such depend is the length of the path forward and backward signal haveto travers in the network the shorter these path between ani combin of posit in the input and output sequenc the easier it isto learn long rang depend 12 henc we also compar the maximum pathlength between ani two input and output posit in network compos of the differ layer type a note in tabl 1a self attent layer connect all posit witha constant numberof sequenti execut oper wherea a recurr layer requir on sequenti oper interm of comput complex self attent layer are faster than recurr layer when the sequenc length n is smaller than the represent dimension d which is most often the case with sentenc represent use by state of the art modelin machin translat sucha word piec 38 and byte pair 31 represent toimprov comput perform for task involv veri long sequenc self attent could be restrict to consid onli a neighborhood of sizer in the input sequenc center around the respect output posit thi would increas the maximum pathlength toon rwe plan to investig thi approach further in futur work asingl convolut layer with kernel width kn doe not connect all pair of input and output posit do so requir a stack ofo nk convolut layer in the case of contigu kernel oro logk nin the case of dilat convolut 18 increas the length of the longest path between ani two posit in the network convolut layer are gener more expens than recurr layer bya factor ofk separ convolut 6 howev decreas the complex consider took ndn d2 even with kn howev the complex ofa separ convolut is equal to the combin ofa self attent layer anda point wise feedforward layer the approach we take in our model a side benefit self attent could yield more interpret model we inspect attent distribut from our model and present and discu exampl in the appendix not onli do individu attent head clearli learn to perform differ task mani appear to exhibit behavior relat to the syntact and semant structur of the sentenc",
    "content_sm_ltks": "3 5posit encod sinc our model contain no recurr and no convolut in order for the model to make useof the order of the sequenc we must inject some inform about the rel or absolut posit of the token in the sequenc tothi end we add posit encod to the input embed at the bottom of the encod and decod stack the posit encod have the same dimens dmodel a the embed sothat thetwo can be sum there are mani choic of posit encod learn and fix 9 inthi work we use sine and cosin function of differ frequenc where pois the posit andi is the dimens that is each dimens of the posit encod correspond toa sinusoid the wavelength forma geometr progress from 2 π to 10000 2 π we chose thi function becaus we hypothes it would allow the model to easili learn to attend by rel posit sinc for ani fix offset k pepo k can be represa a linear function of pepo we also experi with use learn posit embed 9 instead and found that thetwo version produc nearli ident result see tabl 3 row ewe chose the sinusoid version becaus it may allow the model to extrapol to sequenc length longer than the one encount dure train 4whi self attent inthi section we compar variou aspect of self attent layer to the recurr and convolu tional layer commonli usefor map one variabl length sequenc of symbol represent 1 nto anoth sequenc of equal length z1 zn with ci ze rd sucha a hidden layer ina typic sequenc transduct encod or decod motiv our useof self attent we consid three desideratum one is the total comput complex per layer anoth is the amount of comput that can be parallel a measur by the minimum numberof sequenti oper requir the third is the pathlength between long rang depend in the network learn long rang depend isa key challeng in mani sequenc transduct task one key factor affect the abil to learn such depend is the length of the path forward and backward signal haveto travers in the network the shorter these path between ani combin of posit in the input and output sequenc the easier it isto learn long rang depend 12 henc we also compar the maximum pathlength between ani two input and output posit in network compos of the differ layer type a note in tabl 1a self attent layer connect all posit witha constant numberof sequenti execut oper wherea a recurr layer requir on sequenti oper interm of comput complex self attent layer are faster than recurr layer when the sequenc length n is smaller than the represent dimension d which is most often the case with sentenc represent use by state of the art modelin machin translat sucha word piec 38 and byte pair 31 represent toimprov comput perform for task involv veri long sequenc self attent could be restrict to consid onli a neighborhood of sizer in the input sequenc center around the respect output posit thi would increas the maximum pathlength toon rwe plan to investig thi approach further in futur work asingl convolut layer with kernel width kn doe not connect all pair of input and output posit do so requir a stack ofo nk convolut layer in the case of contigu kernel oro logk nin the case of dilat convolut 18 increas the length of the longest path between ani two posit in the network convolut layer are gener more expens than recurr layer bya factor ofk separ convolut 6 howev decreas the complex consider took ndn d2 even with kn howev the complex ofa separ convolut is equal to the combin ofa self attent layer anda point wise feedforward layer the approach we take in our model a side benefit self attent could yield more interpret model we inspect attent distribut from our model and present and discu exampl in the appendix not onli do individu attent head clearli learn to perform differ task mani appear to exhibit behavior relat to the syntact and semant structur of the sentenc"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=688x784 at 0x74CE6E2AD930>",
    "page_num_int": [
      7,
      7
    ],
    "position_int": [
      [
        7,
        105,
        334,
        252,
        266
      ],
      [
        7,
        107,
        336,
        280,
        292
      ]
    ],
    "top_int": [
      252,
      280
    ],
    "content_with_weight": "5 Training\nThis section describes the training regime for our models.",
    "content_ltks": "5 train thi section describ the train regim for our model",
    "content_sm_ltks": "5 train thi section describ the train regim for our model"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1193x1026 at 0x74CE6E2AF130>",
    "page_num_int": [
      7,
      7,
      7,
      7,
      7,
      7,
      7,
      7
    ],
    "position_int": [
      [
        7,
        105,
        503,
        303,
        317
      ],
      [
        7,
        107,
        505,
        326,
        337
      ],
      [
        7,
        107,
        505,
        337,
        349
      ],
      [
        7,
        106,
        504,
        348,
        359
      ],
      [
        7,
        107,
        505,
        359,
        370
      ],
      [
        7,
        108,
        505,
        370,
        382
      ],
      [
        7,
        107,
        505,
        381,
        392
      ],
      [
        7,
        107,
        505,
        392,
        403
      ]
    ],
    "top_int": [
      303,
      326,
      337,
      348,
      359,
      370,
      381,
      392
    ],
    "content_with_weight": "5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.",
    "content_ltks": "51 train data and batch we train on the standard wmt 2014 english german dataset consist of about 45 million sentenc pair sentenc were encod use byte pair encod 3 which haa share sourc target vocabulari of about 37000 token for english french we use the significantli larger wmt 2014 english french dataset consist of 36m sentenc and split token into a 32000 word piec vocabulari 38 sentenc pair were batch togeth by approxim sequenc length each train batch contain aset of sentenc pair contain approxim 25000 sourc token and 25000 target token",
    "content_sm_ltks": "51 train data and batch we train on the standard wmt 2014 english german dataset consist of about 45 million sentenc pair sentenc were encod use byte pair encod 3 which haa share sourc target vocabulari of about 37000 token for english french we use the significantli larger wmt 2014 english french dataset consist of 36m sentenc and split token into a 32000 word piec vocabulari 38 sentenc pair were batch togeth by approxim sequenc length each train batch contain aset of sentenc pair contain approxim 25000 sourc token and 25000 target token"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1198x947 at 0x74CE6E2AD1B0>",
    "page_num_int": [
      7,
      7,
      7,
      7,
      7,
      7
    ],
    "position_int": [
      [
        7,
        105,
        505,
        415,
        427
      ],
      [
        7,
        107,
        506,
        437,
        448
      ],
      [
        7,
        106,
        505,
        447,
        461
      ],
      [
        7,
        107,
        506,
        459,
        471
      ],
      [
        7,
        107,
        506,
        470,
        481
      ],
      [
        7,
        106,
        506,
        480,
        491
      ]
    ],
    "top_int": [
      415,
      437,
      447,
      459,
      470,
      480
    ],
    "content_with_weight": "5.2Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).",
    "content_ltks": "5 2hardwar and schedul we train our model on one machin with 8 nvidia p100 gpu for our base model use the hyperparamet describ throughout the paper each train step took about 0 4 second we train the base model fora total of 100 000 step or 12 hour for our big model describ on the bottom line of tabl 3 step time wa 10 second the big model were train for 300 000 step 35 day",
    "content_sm_ltks": "5 2hardwar and schedul we train our model on one machin with 8 nvidia p100 gpu for our base model use the hyperparamet describ throughout the paper each train step took about 0 4 second we train the base model fora total of 100 000 step or 12 hour for our big model describ on the bottom line of tabl 3 step time wa 10 second the big model were train for 300 000 step 35 day"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x959 at 0x74CE6E2AEE90>",
    "page_num_int": [
      7,
      7,
      7,
      7,
      7,
      7
    ],
    "position_int": [
      [
        7,
        106,
        506,
        505,
        517
      ],
      [
        7,
        106,
        506,
        524,
        538
      ],
      [
        7,
        107,
        507,
        538,
        549
      ],
      [
        7,
        105,
        505,
        585,
        599
      ],
      [
        7,
        105,
        505,
        597,
        611
      ],
      [
        7,
        106,
        506,
        610,
        622
      ]
    ],
    "top_int": [
      505,
      524,
      538,
      585,
      597,
      610
    ],
    "content_with_weight": "5.3Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and e = 10-9. We varied the learning\nrate over the course of training, according to the formula:\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.",
    "content_ltks": "5 3optim we use the adam optim 20 with β 10 9 β 20 98 ande 10 9 we vari the learn rate over the cours of train accord to the formula thi correspond to increas the learn rate linearli for thefirst warmup _ step train step and decreas it thereaft proport to the invers squar root of the step number we use warmup _ step 4000",
    "content_sm_ltks": "5 3optim we use the adam optim 20 with β 10 9 β 20 98 ande 10 9 we vari the learn rate over the cours of train accord to the formula thi correspond to increas the learn rate linearli for thefirst warmup _ step train step and decreas it thereaft proport to the invers squar root of the step number we use warmup _ step 4000"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x1069 at 0x74CE6E2AF730>",
    "page_num_int": [
      7,
      7,
      7,
      7,
      7,
      7,
      8,
      8,
      8
    ],
    "position_int": [
      [
        7,
        106,
        506,
        635,
        646
      ],
      [
        7,
        107,
        507,
        656,
        668
      ],
      [
        7,
        107,
        507,
        679,
        691
      ],
      [
        7,
        105,
        505,
        689,
        703
      ],
      [
        7,
        106,
        506,
        702,
        713
      ],
      [
        7,
        107,
        507,
        712,
        724
      ],
      [
        8,
        106,
        506,
        271,
        283
      ],
      [
        8,
        107,
        507,
        283,
        294
      ],
      [
        8,
        106,
        506,
        312,
        324
      ]
    ],
    "top_int": [
      635,
      656,
      679,
      689,
      702,
      712,
      271,
      283,
      312
    ],
    "content_with_weight": "5.4Regularization\nWe employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel SmoothingDuring training, we employed label smoothing of value Els = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6Results",
    "content_ltks": "5 4regular we employ three typeof regular dure train residu dropout we appli dropout 33 to the output ofeach sub layer befor it isad to the sub layer input and normal inaddit we appli dropout to the sum of the embed and the posit encod in both the encod and decod stack for the base model we usea rate of pdrop 01 label smoothingdur train we employ label smooth of valu el 01 36 thi hurt perplex a the model learn tobe more unsur but improv accuraci and bleu score 6result",
    "content_sm_ltks": "5 4regular we employ three typeof regular dure train residu dropout we appli dropout 33 to the output ofeach sub layer befor it isad to the sub layer input and normal inaddit we appli dropout to the sum of the embed and the posit encod in both the encod and decod stack for the base model we usea rate of pdrop 01 label smoothingdur train we employ label smooth of valu el 01 36 thi hurt perplex a the model learn tobe more unsur but improv accuraci and bleu score 6result"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1200x1516 at 0x74CE6E2BF2B0>",
    "page_num_int": [
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8
    ],
    "position_int": [
      [
        8,
        106,
        506,
        337,
        348
      ],
      [
        8,
        106,
        506,
        358,
        369
      ],
      [
        8,
        106,
        506,
        369,
        380
      ],
      [
        8,
        107,
        507,
        380,
        392
      ],
      [
        8,
        106,
        506,
        391,
        402
      ],
      [
        8,
        107,
        507,
        402,
        414
      ],
      [
        8,
        107,
        507,
        413,
        424
      ],
      [
        8,
        107,
        507,
        428,
        439
      ],
      [
        8,
        106,
        506,
        439,
        451
      ],
      [
        8,
        107,
        507,
        451,
        462
      ],
      [
        8,
        107,
        507,
        462,
        473
      ],
      [
        8,
        107,
        507,
        478,
        490
      ],
      [
        8,
        106,
        506,
        489,
        500
      ],
      [
        8,
        107,
        507,
        500,
        512
      ],
      [
        8,
        107,
        507,
        511,
        523
      ],
      [
        8,
        105,
        505,
        521,
        533
      ],
      [
        8,
        105,
        505,
        536,
        550
      ],
      [
        8,
        106,
        506,
        549,
        561
      ],
      [
        8,
        107,
        507,
        560,
        571
      ],
      [
        8,
        104,
        504,
        570,
        584
      ]
    ],
    "top_int": [
      337,
      358,
      369,
      380,
      391,
      402,
      413,
      428,
      439,
      451,
      462,
      478,
      489,
      500,
      511,
      521,
      536,
      549,
      560,
      570
    ],
    "content_with_weight": "6.1Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty Q = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.",
    "content_ltks": "6 1machin translat on the wmt 2014 english to german translat task the big transform model transform big in tabl 2 outperform thebest previous report model includ ensembl by morethan 20 bleu establish anew state of the art bleu score of 28 4 the configur ofthi modelis listin the bottom line of tabl 3 train took 35 day on 8 p100 gpu even our base model surpass all previous publish model and ensembl ata fraction of the train cost ofani of the competit model on the wmt 2014 english to french translat task our big model achieva bleu score of 41 0 outperform allof the previous publish singl model at lessthan 1 4 the train cost of the previou state of the art model the transform big model train for english to french use dropout rate pdrop 01 instead of 0 3 for the base model we usea singl model obtain by averag thelast 5 checkpoint which were written at 10 minut interv for the big model we averag thelast 20 checkpoint we use beam search witha beam sizeof 4 and length penalti q 0 6 38 these hyperparamet were chosen after experiment on the develop set we set the maximum output length dure infer to input length 50 but termin earli when possibl 38 tabl 2 summar our result and compar our translat qualiti and train cost to other model architectur from the literatur we estim the numberof float point oper use to traina model by multipli the train time the numberof gpu useand an estim of the sustain singl precis float point capac ofeach gpu 5",
    "content_sm_ltks": "6 1machin translat on the wmt 2014 english to german translat task the big transform model transform big in tabl 2 outperform thebest previous report model includ ensembl by morethan 20 bleu establish anew state of the art bleu score of 28 4 the configur ofthi modelis listin the bottom line of tabl 3 train took 35 day on 8 p100 gpu even our base model surpass all previous publish model and ensembl ata fraction of the train cost ofani of the competit model on the wmt 2014 english to french translat task our big model achieva bleu score of 41 0 outperform allof the previous publish singl model at lessthan 1 4 the train cost of the previou state of the art model the transform big model train for english to french use dropout rate pdrop 01 instead of 0 3 for the base model we usea singl model obtain by averag thelast 5 checkpoint which were written at 10 minut interv for the big model we averag thelast 20 checkpoint we use beam search witha beam sizeof 4 and length penalti q 0 6 38 these hyperparamet were chosen after experiment on the develop set we set the maximum output length dure infer to input length 50 but termin earli when possibl 38 tabl 2 summar our result and compar our translat qualiti and train cost to other model architectur from the literatur we estim the numberof float point oper use to traina model by multipli the train time the numberof gpu useand an estim of the sustain singl precis float point capac ofeach gpu 5"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1203x1333 at 0x74CE6E2ACA90>",
    "page_num_int": [
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      8,
      9,
      9,
      9,
      9,
      9,
      9
    ],
    "position_int": [
      [
        8,
        105,
        506,
        596,
        607
      ],
      [
        8,
        104,
        505,
        616,
        630
      ],
      [
        8,
        106,
        507,
        629,
        641
      ],
      [
        8,
        107,
        508,
        640,
        651
      ],
      [
        8,
        106,
        507,
        651,
        663
      ],
      [
        8,
        107,
        508,
        667,
        679
      ],
      [
        8,
        105,
        506,
        677,
        691
      ],
      [
        8,
        107,
        508,
        689,
        701
      ],
      [
        8,
        119,
        520,
        711,
        722
      ],
      [
        9,
        106,
        507,
        597,
        609
      ],
      [
        9,
        104,
        505,
        608,
        622
      ],
      [
        9,
        107,
        508,
        619,
        631
      ],
      [
        9,
        105,
        506,
        628,
        642
      ],
      [
        9,
        106,
        507,
        642,
        653
      ],
      [
        9,
        105,
        506,
        652,
        664
      ]
    ],
    "top_int": [
      596,
      616,
      629,
      640,
      651,
      667,
      677,
      689,
      711,
      597,
      608,
      619,
      628,
      642,
      652
    ],
    "content_with_weight": "6.2Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\nIn Table 3 rows (B), we observe that reducing the attention key size d hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ftting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.",
    "content_ltks": "6 2model variat to evalu the import of differ compon of the transform we vari our base modelin differ way measur the changin perform on english to german translat on the develop set newstest2013 we use beam search a describ in the previou section but no checkpoint averag we present these result in tabl 3 in tabl 3 rowa we vari the numberof attent head and the attent key and valu dimens keep the amount of comput constanta describ in section 32 2 while singl head attent is 0 9 bleu wors than thebest set qualiti also dropoff with too mani head 5we use valu of 28 37 60 and 95 tflop for k80 k40 m40 and p100 respect in tabl 3 row bwe observ that reduc the attent key size d hurt model qualiti thi suggest that determin compat isnot easi andthat a more sophist compat function than dot product maybe benefici we further observ in row cand d thata expect bigger model are better and dropout is veri helpin avoid over ftting in row ewe replac our sinusoid posit encod with learn posit embed 9 and observ nearli ident result to the base model",
    "content_sm_ltks": "6 2model variat to evalu the import of differ compon of the transform we vari our base modelin differ way measur the changin perform on english to german translat on the develop set newstest2013 we use beam search a describ in the previou section but no checkpoint averag we present these result in tabl 3 in tabl 3 rowa we vari the numberof attent head and the attent key and valu dimens keep the amount of comput constanta describ in section 32 2 while singl head attent is 0 9 bleu wors than thebest set qualiti also dropoff with too mani head 5we use valu of 28 37 60 and 95 tflop for k80 k40 m40 and p100 respect in tabl 3 row bwe observ that reduc the attent key size d hurt model qualiti thi suggest that determin compat isnot easi andthat a more sophist compat function than dot product maybe benefici we further observ in row cand d thata expect bigger model are better and dropout is veri helpin avoid over ftting in row ewe replac our sinusoid posit encod with learn posit embed 9 and observ nearli ident result to the base model"
  },
  {
    "docnm_kwd": "Paper on Attention Is All You Need.pdf",
    "authors_tks": "ashish vaswani",
    "title_tks": "attent is all you need",
    "title_sm_tks": "attent is all you need",
    "authors_sm_tks": "ashish vaswani",
    "image": "<PIL.Image.Image image mode=RGB size=1203x6148 at 0x74CE6E2E9450>",
    "page_num_int": [
      9,
      9,
      9,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      10,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      11,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      12,
      13
    ],
    "position_int": [
      [
        9,
        106,
        507,
        679,
        691
      ],
      [
        9,
        104,
        505,
        699,
        713
      ],
      [
        9,
        105,
        506,
        711,
        725
      ],
      [
        10,
        106,
        507,
        74,
        85
      ],
      [
        10,
        106,
        507,
        85,
        96
      ],
      [
        10,
        107,
        508,
        101,
        113
      ],
      [
        10,
        106,
        507,
        112,
        123
      ],
      [
        10,
        106,
        507,
        123,
        134
      ],
      [
        10,
        106,
        507,
        133,
        145
      ],
      [
        10,
        106,
        507,
        144,
        156
      ],
      [
        10,
        107,
        508,
        161,
        173
      ],
      [
        10,
        107,
        508,
        173,
        184
      ],
      [
        10,
        108,
        509,
        184,
        195
      ],
      [
        10,
        107,
        508,
        194,
        206
      ],
      [
        10,
        105,
        506,
        203,
        217
      ],
      [
        10,
        107,
        508,
        221,
        232
      ],
      [
        10,
        107,
        508,
        233,
        245
      ],
      [
        10,
        106,
        507,
        242,
        254
      ],
      [
        10,
        106,
        507,
        259,
        273
      ],
      [
        10,
        107,
        508,
        271,
        283
      ],
      [
        10,
        106,
        507,
        297,
        309
      ],
      [
        10,
        107,
        508,
        321,
        333
      ],
      [
        10,
        107,
        508,
        333,
        345
      ],
      [
        10,
        107,
        508,
        344,
        354
      ],
      [
        10,
        107,
        508,
        359,
        370
      ],
      [
        10,
        107,
        508,
        370,
        382
      ],
      [
        10,
        107,
        508,
        381,
        392
      ],
      [
        10,
        107,
        508,
        393,
        405
      ],
      [
        10,
        108,
        509,
        409,
        420
      ],
      [
        10,
        107,
        508,
        419,
        430
      ],
      [
        10,
        104,
        505,
        429,
        443
      ],
      [
        10,
        107,
        508,
        442,
        453
      ],
      [
        10,
        107,
        508,
        458,
        469
      ],
      [
        10,
        107,
        508,
        468,
        480
      ],
      [
        10,
        108,
        509,
        491,
        503
      ],
      [
        10,
        107,
        508,
        503,
        514
      ],
      [
        10,
        106,
        507,
        528,
        540
      ],
      [
        10,
        110,
        511,
        545,
        558
      ],
      [
        10,
        129,
        530,
        558,
        569
      ],
      [
        10,
        111,
        512,
        575,
        586
      ],
      [
        10,
        129,
        530,
        587,
        599
      ],
      [
        10,
        113,
        514,
        604,
        616
      ],
      [
        10,
        128,
        529,
        615,
        627
      ],
      [
        10,
        113,
        514,
        634,
        646
      ],
      [
        10,
        129,
        530,
        645,
        656
      ],
      [
        10,
        112,
        513,
        662,
        674
      ],
      [
        10,
        128,
        529,
        672,
        686
      ],
      [
        10,
        129,
        530,
        684,
        695
      ],
      [
        10,
        111,
        512,
        699,
        713
      ],
      [
        10,
        127,
        528,
        713,
        725
      ],
      [
        11,
        110,
        511,
        73,
        87
      ],
      [
        11,
        128,
        529,
        85,
        99
      ],
      [
        11,
        113,
        514,
        105,
        117
      ],
      [
        11,
        129,
        530,
        116,
        127
      ],
      [
        11,
        113,
        514,
        136,
        147
      ],
      [
        11,
        127,
        528,
        146,
        160
      ],
      [
        11,
        181,
        582,
        166,
        178
      ],
      [
        11,
        129,
        530,
        178,
        189
      ],
      [
        11,
        107,
        508,
        198,
        208
      ],
      [
        11,
        128,
        529,
        208,
        222
      ],
      [
        11,
        129,
        530,
        220,
        231
      ],
      [
        11,
        108,
        509,
        239,
        250
      ],
      [
        11,
        129,
        530,
        250,
        262
      ],
      [
        11,
        107,
        508,
        269,
        281
      ],
      [
        11,
        129,
        530,
        281,
        292
      ],
      [
        11,
        106,
        507,
        300,
        312
      ],
      [
        11,
        130,
        531,
        312,
        323
      ],
      [
        11,
        128,
        529,
        321,
        335
      ],
      [
        11,
        107,
        508,
        341,
        353
      ],
      [
        11,
        129,
        530,
        354,
        365
      ],
      [
        11,
        105,
        506,
        372,
        386
      ],
      [
        11,
        129,
        530,
        384,
        395
      ],
      [
        11,
        107,
        508,
        404,
        415
      ],
      [
        11,
        129,
        530,
        415,
        426
      ],
      [
        11,
        108,
        509,
        434,
        446
      ],
      [
        11,
        128,
        529,
        444,
        458
      ],
      [
        11,
        129,
        530,
        457,
        466
      ],
      [
        11,
        106,
        507,
        475,
        486
      ],
      [
        11,
        129,
        530,
        486,
        497
      ],
      [
        11,
        108,
        509,
        507,
        518
      ],
      [
        11,
        108,
        509,
        526,
        538
      ],
      [
        11,
        129,
        530,
        537,
        548
      ],
      [
        11,
        107,
        508,
        556,
        567
      ],
      [
        11,
        129,
        530,
        568,
        580
      ],
      [
        11,
        129,
        530,
        579,
        590
      ],
      [
        11,
        108,
        509,
        599,
        610
      ],
      [
        11,
        129,
        530,
        610,
        622
      ],
      [
        11,
        108,
        509,
        629,
        641
      ],
      [
        11,
        129,
        530,
        641,
        652
      ],
      [
        11,
        108,
        509,
        660,
        671
      ],
      [
        11,
        129,
        530,
        671,
        683
      ],
      [
        11,
        108,
        509,
        691,
        703
      ],
      [
        11,
        129,
        530,
        702,
        713
      ],
      [
        11,
        128,
        529,
        713,
        725
      ],
      [
        12,
        107,
        508,
        74,
        85
      ],
      [
        12,
        128,
        529,
        85,
        96
      ],
      [
        12,
        107,
        508,
        114,
        128
      ],
      [
        12,
        128,
        529,
        126,
        137
      ],
      [
        12,
        107,
        508,
        155,
        166
      ],
      [
        12,
        129,
        530,
        166,
        178
      ],
      [
        12,
        129,
        530,
        177,
        189
      ],
      [
        12,
        129,
        530,
        189,
        198
      ],
      [
        12,
        106,
        507,
        217,
        231
      ],
      [
        12,
        127,
        528,
        229,
        241
      ],
      [
        12,
        107,
        508,
        258,
        269
      ],
      [
        12,
        129,
        530,
        269,
        281
      ],
      [
        12,
        106,
        507,
        297,
        309
      ],
      [
        12,
        127,
        528,
        308,
        322
      ],
      [
        12,
        129,
        530,
        321,
        332
      ],
      [
        12,
        107,
        508,
        349,
        360
      ],
      [
        12,
        129,
        530,
        361,
        373
      ],
      [
        12,
        129,
        530,
        372,
        383
      ],
      [
        12,
        107,
        508,
        401,
        413
      ],
      [
        12,
        129,
        530,
        412,
        424
      ],
      [
        12,
        126,
        527,
        421,
        435
      ],
      [
        12,
        129,
        530,
        435,
        444
      ],
      [
        12,
        108,
        509,
        464,
        476
      ],
      [
        12,
        129,
        530,
        476,
        487
      ],
      [
        12,
        107,
        508,
        505,
        516
      ],
      [
        12,
        128,
        529,
        515,
        527
      ],
      [
        12,
        107,
        508,
        545,
        557
      ],
      [
        12,
        126,
        527,
        554,
        568
      ],
      [
        12,
        106,
        507,
        585,
        596
      ],
      [
        12,
        128,
        529,
        595,
        607
      ],
      [
        12,
        129,
        530,
        608,
        619
      ],
      [
        12,
        128,
        529,
        618,
        628
      ],
      [
        12,
        108,
        509,
        648,
        660
      ],
      [
        12,
        129,
        530,
        659,
        670
      ],
      [
        12,
        107,
        508,
        689,
        700
      ],
      [
        12,
        129,
        530,
        700,
        711
      ],
      [
        12,
        129,
        530,
        711,
        722
      ],
      [
        13,
        108,
        509,
        72,
        84
      ]
    ],
    "top_int": [
      679,
      699,
      711,
      74,
      85,
      101,
      112,
      123,
      133,
      144,
      161,
      173,
      184,
      194,
      203,
      221,
      233,
      242,
      259,
      271,
      297,
      321,
      333,
      344,
      359,
      370,
      381,
      393,
      409,
      419,
      429,
      442,
      458,
      468,
      491,
      503,
      528,
      545,
      558,
      575,
      587,
      604,
      615,
      634,
      645,
      662,
      672,
      684,
      699,
      713,
      73,
      85,
      105,
      116,
      136,
      146,
      166,
      178,
      198,
      208,
      220,
      239,
      250,
      269,
      281,
      300,
      312,
      321,
      341,
      354,
      372,
      384,
      404,
      415,
      434,
      444,
      457,
      475,
      486,
      507,
      526,
      537,
      556,
      568,
      579,
      599,
      610,
      629,
      641,
      660,
      671,
      691,
      702,
      713,
      74,
      85,
      114,
      126,
      155,
      166,
      177,
      189,
      217,
      229,
      258,
      269,
      297,
      308,
      321,
      349,
      361,
      372,
      401,
      412,
      421,
      435,
      464,
      476,
      505,
      515,
      545,
      554,
      585,
      595,
      608,
      618,
      648,
      659,
      689,
      700,
      711,
      72
    ],
    "content_with_weight": "6.3English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\nincreased the maximum output length to input length + 300. We used a beam size of 21 and Q = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to effciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\ns.- Generating sequences with recurrent neural networks. arXiv preprint[10] Alex Graves.\narXiv:1308.0850,2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770-778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber. Gradient fow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Juirgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735-1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2o09 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832-841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Lukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv: 1610. 10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv: 1508.04025, 2015.\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152-159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433-440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929-1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates,\nInc.,2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144,2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434-443. ACL, August 2013.\nAttentionVisualizations",
    "content_ltks": "6 3english constitu pars to evalu if the transform can gener to other task we perform experi on english constitu pars thi task present specif challeng the output is subject to strong structur constraint and is significantli longer than the input furthermor rnn sequenc to sequenc model havenot been abl to attain state of the art result in small data regim 37 we traina 4 layer transform with dmodel 1024 on the wallstreet journal wsj portion of the penn treebank 25 about 40k train sentenc we also train itin a semi supervis set use the larger high confid and berkleypars corpu from with approxim 17m sentenc 37 we usea vocabulari of 16k token for the wsj onli set anda vocabulari of 32k token for the semi supervis set we perform onli a small numberof experi to select the dropout both attent and residu section 54 learn rate and beam size on the section 22 develop set all other paramet remain unchang from the english to german base translat model dure infer we increas the maximum output length to input length 300 we usea beam sizeof 21 andq 0 3 for both wsj onli and the semi supervis set our result in tabl 4 show that despit the lack of task specif tune our model perform surprisingli well yield better result than all previous report model with the except of the recurr neural network grammar 8 in contrast to rnn sequenc to sequenc model 37 the transform outperform the berkeley parser 29 even when train onlion the wsj train setof 40k sentenc 7conclus inthi work we present the transform thefirst sequenc transduct model base entir on attent replac the recurr layer most commonli use in encod decod architectur with multihead self attent for translat task the transform can be train significantli faster than architectur base on recurr or convolut layeron both wmt 2014 english to german and wmt 2014 english to french translat task we achieva new state of the artin the former task our best model outperform even all previous report ensembl we are excit about the futur of attent base model and plan to appli them to other task we plan to extend the transform to problem involv input and output modal other than text andto investig local restrict attent mechan to effcient handl larg input and output sucha imag audio and video make gener less sequenti is anoth research goal ofour the code we use to train and evalu our modelis avail athttp github com tensorflow tensor2tensor acknowledg we are grate tonal kalchbrenn and stephan gouw for their fruit comment correct and inspir refer 1 jimmi leiba jami ryan kiro and geoffrey e hinton layer normal arxiv preprint arxiv 1607 06450 2016 2 dzmitri bahdanau kyunghyun cho and yoshua bengio neural machin translat by jointli learn to align and translat corr ab 1409 0473 2014 3 denni britz anna goldi minh thang luong and quoc vle massiv explor of neural machin translat architectur corr ab 1703 03906 2017 4 jianpeng cheng li dong and mirella lapata long shortterm memori network for machin read arxiv preprint arxiv 1601 06733 2016 5 kyunghyun cho bart van merrienbo caglar gulcehr fethi bougar holger schwenk and yoshua bengio learn phrase represent use rnn encod decod for statist machin translat corr ab 1406 1078 2014 6 francoi chollet xception deep learn with depthwis separ convolut arxiv preprint arxiv 1610 02357 2016 7 junyoung chung caglar gulcehr kyunghyun cho and yoshua bengio empir evalu of gate recurr neural network on sequenc model corr ab 1412 3555 2014 8 chri dyer adhiguna kuncoro miguel ballestero and noah a smith recurr neural network grammar in proc of naacl 2016 9 jona gehr michael auli david grangier deni yarat and yann n dauphin convolu tional sequenc to sequenc learn arxiv preprint arxiv 1705 03122v2 2017 s gener sequenc with recurr neural network arxiv preprint 10 alex graf arxiv 1308 0850 2013 11 kaim he xiangyu zhang shaoq ren and jian sundeep residu learn for im age recognit inproceed of the ieee confer on comput vision and pattern recognit page 770 778 2016 12 sepp hochreit yoshua bengio paolo frasconi and jurgen schmidhub gradient fow in recurr net the difficulti of learn longterm depend 2001 13 sepp hochreit and juirgen schmidhub long shortterm memori neural comput 98 1735 1780 1997 14 zhongqiang huang and mari harper self train pcfg grammar with latent annot across languag inproceed of the 2o09 confer on empir method in natur languag process page 832 841 acl august 2009 15 rafal jozefowicz oriol vinyal mike schuster noam shazeer and yonghui wu explor the limit of languag model arxiv preprint arxiv 1602 02410 2016 16 lukasz kaiser and sami bengio can activ memori replac attent in advanc in neural inform process system nip 2016 17 lukasz kaiser and ilya sutskev neural gpu learn algorithm in intern confer on learn represent iclr 2016 18 nal kalchbrenn lass espeholt karen simonyan aaron vanden oord alex graf and koray kavukcuoglu neural machin translat in linear time arxiv preprint arxiv 1610 10099v2 2017 19 yoon kim carl denton luong hoang and alexand m rush structur attent network in intern confer on learn represent 2017 20 diederik kingma and jimmi ba adama method for stochast optim in iclr 2015 21 oleksii kuchaiev and bori ginsburg factor trick for lstm network arxiv preprint arxiv 1703 10722 2017 22 zhouhan lin minwei feng cicero nogueira do santo mo yu bing xiang bowen zhou and yoshua bengio a structur self attent sentenc embed arxiv preprint arxiv 1703 03130 2017 23 minh thang luong quoc vle ilya sutskev oriol vinyal and lukasz kaiser multitask sequenc to sequenc learn arxiv preprint arxiv 1511 06114 2015 24 minh thang luong hieu pham and christoph dman effect approach to attent base neural machin translat arxiv preprint arxiv 1508 04025 2015 25 mitchel p marcu mariann marcinkiewicz and beatric santorini build a larg annot corpu of english the penn treebank comput linguist 19 2 313 330 1993 26 david mccloski eugen charniak and mark johnson effect self train for pars inproceed of the human languag technolog confer of the naacl main confer page 152 159 acl june 2006 27 ankur parikh oscar tackstrom dipanjan da and jakob uszkoreit a decompos attent modelin empir method in natur languag process 2016 28 romain paulu caim xiong and richard socher a deep reinforc model for abstract summar arxiv preprint arxiv 1705 04304 2017 29 slav petrov leon barrett romain thibaux and dan klein learn accur compact and interpret tree annot inproceed of the 21st intern confer on comput linguist and 44th annual meet of the acl page 433 440 acl juli 2006 30 ofir press and lior wolf use the output embed toimprov languag model arxiv preprint arxiv 1608 05859 2016 31 rico sennrich barri haddow and alexandra birch neural machin translat of rare word with subword unit arxiv preprint arxiv 1508 07909 2015 32 noam shazeer azalia mirhoseini krzysztof maziarz andi davi quoc le geoffrey hinton and jeff dean outrag larg neural network the spars gate mixtur of expert layer arxiv preprint arxiv 1701 06538 2017 33 nitish srivastava geoffrey e hinton alex krizhevski ilya sutskev and ruslan salakhutdi nov dropout a simpl wayto prevent neural network from overfit journal of machin learn research 15 1 1929 1958 2014 34 sainbayar sukhbaatar arthur szlam jason weston and rob fergu end to end memori network inc cort nd lawrenc dd leem sugiyama andr garnett editor advanc in neural inform process system 28 page 2440 2448 curran associ inc 2015 35 ilya sutskev oriol vinyal and quoc vv le sequenc to sequenc learn with neural network in advanc in neural inform process system page 3104 3112 2014 36 christian szegedi vincent vanhouck sergey ioff jonathon shlen and zbigniew wojna rethink the incept architectur for comput vision corr ab 1512 00567 2015 37 vinyal kaiser koo petrov sutskev and hinton grammar aa foreign languag in advanc in neural inform process system 2015 38 yonghui wu mike schuster zhifeng chen quoc vle mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klau macherey etal googl s neural machin translat system bridg the gap between human and machin translat arxiv preprint arxiv 1609 08144 2016 39 jie zhou ying cao xuguang wang peng li and wei xu deep recurr model with fastforward connect for neural machin translat corr ab 1606 04199 2016 40 muhua zhu yue zhang wenliang chen min zhang and jingbo zhu fast and accur shift reduc constitu pars inproceed of the 51st annual meet of the acl volum 1 long paper page 434 443 acl august 2013 attentionvisu",
    "content_sm_ltks": "6 3english constitu pars to evalu if the transform can gener to other task we perform experi on english constitu pars thi task present specif challeng the output is subject to strong structur constraint and is significantli longer than the input furthermor rnn sequenc to sequenc model havenot been abl to attain state of the art result in small data regim 37 we traina 4 layer transform with dmodel 1024 on the wallstreet journal wsj portion of the penn treebank 25 about 40k train sentenc we also train itin a semi supervis set use the larger high confid and berkleypars corpu from with approxim 17m sentenc 37 we usea vocabulari of 16k token for the wsj onli set anda vocabulari of 32k token for the semi supervis set we perform onli a small numberof experi to select the dropout both attent and residu section 54 learn rate and beam size on the section 22 develop set all other paramet remain unchang from the english to german base translat model dure infer we increas the maximum output length to input length 300 we usea beam sizeof 21 andq 0 3 for both wsj onli and the semi supervis set our result in tabl 4 show that despit the lack of task specif tune our model perform surprisingli well yield better result than all previous report model with the except of the recurr neural network grammar 8 in contrast to rnn sequenc to sequenc model 37 the transform outperform the berkeley parser 29 even when train onlion the wsj train setof 40k sentenc 7conclus inthi work we present the transform thefirst sequenc transduct model base entir on attent replac the recurr layer most commonli use in encod decod architectur with multihead self attent for translat task the transform can be train significantli faster than architectur base on recurr or convolut layeron both wmt 2014 english to german and wmt 2014 english to french translat task we achieva new state of the artin the former task our best model outperform even all previous report ensembl we are excit about the futur of attent base model and plan to appli them to other task we plan to extend the transform to problem involv input and output modal other than text andto investig local restrict attent mechan to effcient handl larg input and output sucha imag audio and video make gener less sequenti is anoth research goal ofour the code we use to train and evalu our modelis avail athttp github com tensorflow tensor2tensor acknowledg we are grate tonal kalchbrenn and stephan gouw for their fruit comment correct and inspir refer 1 jimmi leiba jami ryan kiro and geoffrey e hinton layer normal arxiv preprint arxiv 1607 06450 2016 2 dzmitri bahdanau kyunghyun cho and yoshua bengio neural machin translat by jointli learn to align and translat corr ab 1409 0473 2014 3 denni britz anna goldi minh thang luong and quoc vle massiv explor of neural machin translat architectur corr ab 1703 03906 2017 4 jianpeng cheng li dong and mirella lapata long shortterm memori network for machin read arxiv preprint arxiv 1601 06733 2016 5 kyunghyun cho bart van merrienbo caglar gulcehr fethi bougar holger schwenk and yoshua bengio learn phrase represent use rnn encod decod for statist machin translat corr ab 1406 1078 2014 6 francoi chollet xception deep learn with depthwis separ convolut arxiv preprint arxiv 1610 02357 2016 7 junyoung chung caglar gulcehr kyunghyun cho and yoshua bengio empir evalu of gate recurr neural network on sequenc model corr ab 1412 3555 2014 8 chri dyer adhiguna kuncoro miguel ballestero and noah a smith recurr neural network grammar in proc of naacl 2016 9 jona gehr michael auli david grangier deni yarat and yann n dauphin convolu tional sequenc to sequenc learn arxiv preprint arxiv 1705 03122v2 2017 s gener sequenc with recurr neural network arxiv preprint 10 alex graf arxiv 1308 0850 2013 11 kaim he xiangyu zhang shaoq ren and jian sundeep residu learn for im age recognit inproceed of the ieee confer on comput vision and pattern recognit page 770 778 2016 12 sepp hochreit yoshua bengio paolo frasconi and jurgen schmidhub gradient fow in recurr net the difficulti of learn longterm depend 2001 13 sepp hochreit and juirgen schmidhub long shortterm memori neural comput 98 1735 1780 1997 14 zhongqiang huang and mari harper self train pcfg grammar with latent annot across languag inproceed of the 2o09 confer on empir method in natur languag process page 832 841 acl august 2009 15 rafal jozefowicz oriol vinyal mike schuster noam shazeer and yonghui wu explor the limit of languag model arxiv preprint arxiv 1602 02410 2016 16 lukasz kaiser and sami bengio can activ memori replac attent in advanc in neural inform process system nip 2016 17 lukasz kaiser and ilya sutskev neural gpu learn algorithm in intern confer on learn represent iclr 2016 18 nal kalchbrenn lass espeholt karen simonyan aaron vanden oord alex graf and koray kavukcuoglu neural machin translat in linear time arxiv preprint arxiv 1610 10099v2 2017 19 yoon kim carl denton luong hoang and alexand m rush structur attent network in intern confer on learn represent 2017 20 diederik kingma and jimmi ba adama method for stochast optim in iclr 2015 21 oleksii kuchaiev and bori ginsburg factor trick for lstm network arxiv preprint arxiv 1703 10722 2017 22 zhouhan lin minwei feng cicero nogueira do santo mo yu bing xiang bowen zhou and yoshua bengio a structur self attent sentenc embed arxiv preprint arxiv 1703 03130 2017 23 minh thang luong quoc vle ilya sutskev oriol vinyal and lukasz kaiser multitask sequenc to sequenc learn arxiv preprint arxiv 1511 06114 2015 24 minh thang luong hieu pham and christoph dman effect approach to attent base neural machin translat arxiv preprint arxiv 1508 04025 2015 25 mitchel p marcu mariann marcinkiewicz and beatric santorini build a larg annot corpu of english the penn treebank comput linguist 19 2 313 330 1993 26 david mccloski eugen charniak and mark johnson effect self train for pars inproceed of the human languag technolog confer of the naacl main confer page 152 159 acl june 2006 27 ankur parikh oscar tackstrom dipanjan da and jakob uszkoreit a decompos attent modelin empir method in natur languag process 2016 28 romain paulu caim xiong and richard socher a deep reinforc model for abstract summar arxiv preprint arxiv 1705 04304 2017 29 slav petrov leon barrett romain thibaux and dan klein learn accur compact and interpret tree annot inproceed of the 21st intern confer on comput linguist and 44th annual meet of the acl page 433 440 acl juli 2006 30 ofir press and lior wolf use the output embed toimprov languag model arxiv preprint arxiv 1608 05859 2016 31 rico sennrich barri haddow and alexandra birch neural machin translat of rare word with subword unit arxiv preprint arxiv 1508 07909 2015 32 noam shazeer azalia mirhoseini krzysztof maziarz andi davi quoc le geoffrey hinton and jeff dean outrag larg neural network the spars gate mixtur of expert layer arxiv preprint arxiv 1701 06538 2017 33 nitish srivastava geoffrey e hinton alex krizhevski ilya sutskev and ruslan salakhutdi nov dropout a simpl wayto prevent neural network from overfit journal of machin learn research 15 1 1929 1958 2014 34 sainbayar sukhbaatar arthur szlam jason weston and rob fergu end to end memori network inc cort nd lawrenc dd leem sugiyama andr garnett editor advanc in neural inform process system 28 page 2440 2448 curran associ inc 2015 35 ilya sutskev oriol vinyal and quoc vv le sequenc to sequenc learn with neural network in advanc in neural inform process system page 3104 3112 2014 36 christian szegedi vincent vanhouck sergey ioff jonathon shlen and zbigniew wojna rethink the incept architectur for comput vision corr ab 1512 00567 2015 37 vinyal kaiser koo petrov sutskev and hinton grammar aa foreign languag in advanc in neural inform process system 2015 38 yonghui wu mike schuster zhifeng chen quoc vle mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klau macherey etal googl s neural machin translat system bridg the gap between human and machin translat arxiv preprint arxiv 1609 08144 2016 39 jie zhou ying cao xuguang wang peng li and wei xu deep recurr model with fastforward connect for neural machin translat corr ab 1606 04199 2016 40 muhua zhu yue zhang wenliang chen min zhang and jingbo zhu fast and accur shift reduc constitu pars inproceed of the 51st annual meet of the acl volum 1 long paper page 434 443 acl august 2013 attentionvisu"
  }
]